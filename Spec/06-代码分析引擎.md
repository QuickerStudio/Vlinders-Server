# Vlinders-Server ä»£ç åˆ†æå¼•æ“

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-28
**æ–‡æ¡£ç±»å‹**: æŠ€æœ¯è®¾è®¡

---

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ Vlinders-Server çš„ä»£ç åˆ†æå¼•æ“è®¾è®¡ï¼ŒåŒ…æ‹¬ Tree-sitter é›†æˆã€è¯­æ³•è§£æã€ç¬¦å·æå–ã€ä¾èµ–åˆ†æã€ä»£ç åµŒå…¥å’Œè¯­ä¹‰æœç´¢ã€‚

---

## ğŸ¯ è®¾è®¡ç›®æ ‡

1. **é«˜æ€§èƒ½** - æ”¯æŒå¤§å‹ä»£ç åº“çš„å¿«é€Ÿè§£æ
2. **å¤šè¯­è¨€** - æ”¯æŒä¸»æµç¼–ç¨‹è¯­è¨€ï¼ˆPythonã€JavaScriptã€TypeScriptã€Goã€Rust ç­‰ï¼‰
3. **å¢é‡æ›´æ–°** - æ–‡ä»¶ä¿®æ”¹æ—¶åªé‡æ–°è§£æå˜æ›´éƒ¨åˆ†
4. **è¯­ä¹‰ç†è§£** - æå–ç¬¦å·ã€ä¾èµ–å…³ç³»ã€è°ƒç”¨å›¾
5. **å‘é‡æœç´¢** - ç”Ÿæˆä»£ç åµŒå…¥ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢

---

## ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ä»£ç åˆ†æå¼•æ“                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Tree-sitter  â”‚  â”‚  ç¬¦å·æå–å™¨   â”‚  â”‚  ä¾èµ–åˆ†æå™¨   â”‚      â”‚
â”‚  â”‚   è§£æå™¨     â”‚â”€â–¶â”‚              â”‚â”€â–¶â”‚              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                  â”‚                  â”‚              â”‚
â”‚         â–¼                  â–¼                  â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  AST ç¼“å­˜    â”‚  â”‚  ç¬¦å·ç´¢å¼•    â”‚  â”‚  è°ƒç”¨å›¾      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                  â”‚                  â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                            â”‚                                  â”‚
â”‚                            â–¼                                  â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                   â”‚  åµŒå…¥ç”Ÿæˆå™¨   â”‚                           â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                            â”‚                                  â”‚
â”‚                            â–¼                                  â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                   â”‚    Qdrant    â”‚                           â”‚
â”‚                   â”‚  å‘é‡æ•°æ®åº“   â”‚                           â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ³ Tree-sitter é›†æˆ

### 1. ä¸ºä»€ä¹ˆé€‰æ‹© Tree-sitter

æ ¹æ® [02-æŠ€æœ¯é€‰å‹.md](./02-æŠ€æœ¯é€‰å‹.md)ï¼ŒTree-sitter çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

- **é«˜æ€§èƒ½** - C å®ç°ï¼Œè§£æé€Ÿåº¦å¿«
- **å¢é‡è§£æ** - åªé‡æ–°è§£æä¿®æ”¹çš„éƒ¨åˆ†ï¼Œæ€§èƒ½æå‡ 10x
- **é”™è¯¯æ¢å¤** - å³ä½¿ä»£ç æœ‰è¯­æ³•é”™è¯¯ä¹Ÿèƒ½è§£æ
- **å¤šè¯­è¨€æ”¯æŒ** - 40+ è¯­è¨€ï¼Œç»Ÿä¸€ API
- **æ˜“äºé›†æˆ** - Python ç»‘å®šå®Œå–„

### 2. å®‰è£…ä¸é…ç½®

#### 2.1 ä¾èµ–å®‰è£…

```bash
# å®‰è£… Tree-sitter Python ç»‘å®š
pip install tree-sitter

# å®‰è£…è¯­è¨€è§£æå™¨
pip install tree-sitter-python
pip install tree-sitter-javascript
pip install tree-sitter-typescript
pip install tree-sitter-go
pip install tree-sitter-rust
pip install tree-sitter-java
```

#### 2.2 è§£æå™¨ç®¡ç†å™¨

```python
# vlinders_server/code_analysis/parser_manager.py

from typing import Dict, Optional
from pathlib import Path
import tree_sitter
from tree_sitter import Language, Parser

class ParserManager:
    """Tree-sitter è§£æå™¨ç®¡ç†å™¨"""

    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # è¯­è¨€åˆ°è§£æå™¨çš„æ˜ å°„
        self._parsers: Dict[str, Parser] = {}
        self._languages: Dict[str, Language] = {}

        # åˆå§‹åŒ–æ”¯æŒçš„è¯­è¨€
        self._init_languages()

    def _init_languages(self):
        """åˆå§‹åŒ–è¯­è¨€è§£æå™¨"""
        # è¯­è¨€é…ç½®
        language_configs = {
            'python': 'tree-sitter-python',
            'javascript': 'tree-sitter-javascript',
            'typescript': 'tree-sitter-typescript/typescript',
            'tsx': 'tree-sitter-typescript/tsx',
            'go': 'tree-sitter-go',
            'rust': 'tree-sitter-rust',
            'java': 'tree-sitter-java',
            'c': 'tree-sitter-c',
            'cpp': 'tree-sitter-cpp',
            'csharp': 'tree-sitter-c-sharp',
        }

        for lang_name, repo_path in language_configs.items():
            try:
                # æ„å»ºè¯­è¨€åº“
                language = Language(
                    self._build_library(lang_name, repo_path),
                    lang_name
                )
                self._languages[lang_name] = language

                # åˆ›å»ºè§£æå™¨
                parser = Parser()
                parser.set_language(language)
                self._parsers[lang_name] = parser

            except Exception as e:
                print(f"Failed to load {lang_name}: {e}")

    def _build_library(self, lang_name: str, repo_path: str) -> str:
        """æ„å»ºè¯­è¨€åº“"""
        library_path = self.cache_dir / f"{lang_name}.so"

        if not library_path.exists():
            # ä» GitHub å…‹éš†å¹¶æ„å»º
            Language.build_library(
                str(library_path),
                [f'vendor/{repo_path}']
            )

        return str(library_path)

    def get_parser(self, language: str) -> Optional[Parser]:
        """è·å–æŒ‡å®šè¯­è¨€çš„è§£æå™¨"""
        return self._parsers.get(language)

    def get_language(self, language: str) -> Optional[Language]:
        """è·å–æŒ‡å®šè¯­è¨€å¯¹è±¡"""
        return self._languages.get(language)

    def detect_language(self, file_path: Path) -> Optional[str]:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åæ£€æµ‹è¯­è¨€"""
        ext_to_lang = {
            '.py': 'python',
            '.js': 'javascript',
            '.jsx': 'javascript',
            '.ts': 'typescript',
            '.tsx': 'tsx',
            '.go': 'go',
            '.rs': 'rust',
            '.java': 'java',
            '.c': 'c',
            '.h': 'c',
            '.cpp': 'cpp',
            '.cc': 'cpp',
            '.cxx': 'cpp',
            '.hpp': 'cpp',
            '.cs': 'csharp',
        }

        return ext_to_lang.get(file_path.suffix.lower())
```

---

## ğŸ” è¯­æ³•è§£æ

### 1. AST ç”Ÿæˆ

```python
# vlinders_server/code_analysis/ast_parser.py

from typing import Optional, Dict, Any
from pathlib import Path
from tree_sitter import Tree, Node
from .parser_manager import ParserManager

class ASTParser:
    """AST è§£æå™¨"""

    def __init__(self, parser_manager: ParserManager):
        self.parser_manager = parser_manager
        # AST ç¼“å­˜ï¼šfile_path -> (content_hash, tree)
        self._ast_cache: Dict[str, tuple[str, Tree]] = {}

    def parse_file(self, file_path: Path, force: bool = False) -> Optional[Tree]:
        """è§£ææ–‡ä»¶ç”Ÿæˆ AST"""
        # æ£€æµ‹è¯­è¨€
        language = self.parser_manager.detect_language(file_path)
        if not language:
            return None

        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            content = file_path.read_text(encoding='utf-8')
        except Exception as e:
            print(f"Failed to read {file_path}: {e}")
            return None

        # æ£€æŸ¥ç¼“å­˜
        content_hash = self._hash_content(content)
        cache_key = str(file_path)

        if not force and cache_key in self._ast_cache:
            cached_hash, cached_tree = self._ast_cache[cache_key]
            if cached_hash == content_hash:
                return cached_tree

        # è§£æä»£ç 
        parser = self.parser_manager.get_parser(language)
        if not parser:
            return None

        tree = parser.parse(bytes(content, 'utf-8'))

        # ç¼“å­˜ç»“æœ
        self._ast_cache[cache_key] = (content_hash, tree)

        return tree

    def parse_code(self, code: str, language: str) -> Optional[Tree]:
        """è§£æä»£ç å­—ç¬¦ä¸²"""
        parser = self.parser_manager.get_parser(language)
        if not parser:
            return None

        return parser.parse(bytes(code, 'utf-8'))

    def _hash_content(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        import hashlib
        return hashlib.sha256(content.encode()).hexdigest()

    def get_node_text(self, node: Node, source_code: bytes) -> str:
        """è·å–èŠ‚ç‚¹çš„æ–‡æœ¬å†…å®¹"""
        return source_code[node.start_byte:node.end_byte].decode('utf-8')

    def traverse_tree(self, tree: Tree, visitor_fn):
        """éå† AST"""
        def _traverse(node: Node):
            visitor_fn(node)
            for child in node.children:
                _traverse(child)

        _traverse(tree.root_node)
```

### 2. å¢é‡è§£æ

```python
# vlinders_server/code_analysis/incremental_parser.py

from typing import Optional, List, Tuple
from tree_sitter import Tree, Parser
from dataclasses import dataclass

@dataclass
class Edit:
    """ä»£ç ç¼–è¾‘æ“ä½œ"""
    start_byte: int
    old_end_byte: int
    new_end_byte: int
    start_point: Tuple[int, int]  # (row, column)
    old_end_point: Tuple[int, int]
    new_end_point: Tuple[int, int]

class IncrementalParser:
    """å¢é‡è§£æå™¨"""

    def __init__(self, parser: Parser):
        self.parser = parser
        self._old_tree: Optional[Tree] = None
        self._old_content: Optional[bytes] = None

    def parse_initial(self, content: str) -> Tree:
        """åˆå§‹è§£æ"""
        content_bytes = bytes(content, 'utf-8')
        self._old_content = content_bytes
        self._old_tree = self.parser.parse(content_bytes)
        return self._old_tree

    def parse_edit(self, new_content: str, edits: List[Edit]) -> Tree:
        """å¢é‡è§£æç¼–è¾‘åçš„ä»£ç """
        if not self._old_tree:
            return self.parse_initial(new_content)

        # åº”ç”¨ç¼–è¾‘æ“ä½œåˆ°æ—§æ ‘
        for edit in edits:
            self._old_tree.edit(
                start_byte=edit.start_byte,
                old_end_byte=edit.old_end_byte,
                new_end_byte=edit.new_end_byte,
                start_point=edit.start_point,
                old_end_point=edit.old_end_point,
                new_end_point=edit.new_end_point,
            )

        # å¢é‡è§£æ
        new_content_bytes = bytes(new_content, 'utf-8')
        new_tree = self.parser.parse(new_content_bytes, self._old_tree)

        # æ›´æ–°ç¼“å­˜
        self._old_content = new_content_bytes
        self._old_tree = new_tree

        return new_tree

    def get_changed_ranges(self, new_tree: Tree) -> List[Tuple[int, int]]:
        """è·å–å˜æ›´çš„ä»£ç èŒƒå›´"""
        if not self._old_tree:
            return []

        changed_ranges = []
        for range in self._old_tree.changed_ranges(new_tree):
            changed_ranges.append((range.start_byte, range.end_byte))

        return changed_ranges
```

### 3. é”™è¯¯æ¢å¤

```python
# vlinders_server/code_analysis/error_recovery.py

from typing import List, Optional
from tree_sitter import Node, Tree
from dataclasses import dataclass

@dataclass
class ParseError:
    """è§£æé”™è¯¯"""
    node: Node
    message: str
    start_line: int
    end_line: int
    severity: str  # 'error' | 'warning'

class ErrorRecovery:
    """é”™è¯¯æ¢å¤å¤„ç†"""

    def find_errors(self, tree: Tree, source_code: bytes) -> List[ParseError]:
        """æŸ¥æ‰¾ AST ä¸­çš„é”™è¯¯èŠ‚ç‚¹"""
        errors = []

        def _find_error_nodes(node: Node):
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯èŠ‚ç‚¹
            if node.type == 'ERROR' or node.is_missing:
                errors.append(ParseError(
                    node=node,
                    message=self._get_error_message(node, source_code),
                    start_line=node.start_point[0],
                    end_line=node.end_point[0],
                    severity='error' if node.type == 'ERROR' else 'warning'
                ))

            # é€’å½’æ£€æŸ¥å­èŠ‚ç‚¹
            for child in node.children:
                _find_error_nodes(child)

        _find_error_nodes(tree.root_node)
        return errors

    def _get_error_message(self, node: Node, source_code: bytes) -> str:
        """ç”Ÿæˆé”™è¯¯æ¶ˆæ¯"""
        if node.is_missing:
            return f"Missing {node.type}"

        text = source_code[node.start_byte:node.end_byte].decode('utf-8', errors='ignore')
        return f"Syntax error near: {text[:50]}"

    def can_extract_symbols(self, tree: Tree) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¯ä»¥æå–ç¬¦å·ï¼ˆå³ä½¿æœ‰é”™è¯¯ï¼‰"""
        # å¦‚æœæ ¹èŠ‚ç‚¹ä¸æ˜¯ ERRORï¼Œé€šå¸¸å¯ä»¥æå–éƒ¨åˆ†ç¬¦å·
        return tree.root_node.type != 'ERROR'
```

---

## ğŸ”– ç¬¦å·æå–

### 1. ç¬¦å·å®šä¹‰

```python
# vlinders_server/code_analysis/symbols.py

from typing import List, Optional, Dict, Any
from dataclasses import dataclass, field
from enum import Enum

class SymbolKind(Enum):
    """ç¬¦å·ç±»å‹"""
    FUNCTION = "function"
    CLASS = "class"
    METHOD = "method"
    VARIABLE = "variable"
    CONSTANT = "constant"
    IMPORT = "import"
    INTERFACE = "interface"
    ENUM = "enum"
    TYPE_ALIAS = "type_alias"

@dataclass
class Symbol:
    """ä»£ç ç¬¦å·"""
    name: str
    kind: SymbolKind
    file_path: str
    start_line: int
    end_line: int
    start_byte: int
    end_byte: int

    # å¯é€‰å­—æ®µ
    docstring: Optional[str] = None
    signature: Optional[str] = None
    parent: Optional[str] = None  # çˆ¶ç¬¦å·åç§°ï¼ˆå¦‚ç±»åï¼‰
    modifiers: List[str] = field(default_factory=list)  # public, private, static ç­‰
    parameters: List[Dict[str, Any]] = field(default_factory=list)
    return_type: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'name': self.name,
            'kind': self.kind.value,
            'file_path': self.file_path,
            'start_line': self.start_line,
            'end_line': self.end_line,
            'docstring': self.docstring,
            'signature': self.signature,
            'parent': self.parent,
            'modifiers': self.modifiers,
        }
```

### 2. Python ç¬¦å·æå–å™¨

```python
# vlinders_server/code_analysis/extractors/python_extractor.py

from typing import List
from tree_sitter import Node, Tree
from ..symbols import Symbol, SymbolKind

class PythonSymbolExtractor:
    """Python ç¬¦å·æå–å™¨"""

    def __init__(self, source_code: bytes):
        self.source_code = source_code

    def extract(self, tree: Tree, file_path: str) -> List[Symbol]:
        """æå–æ‰€æœ‰ç¬¦å·"""
        symbols = []

        def _extract_from_node(node: Node, parent_name: Optional[str] = None):
            # æå–å‡½æ•°å®šä¹‰
            if node.type == 'function_definition':
                symbol = self._extract_function(node, file_path, parent_name)
                if symbol:
                    symbols.append(symbol)

            # æå–ç±»å®šä¹‰
            elif node.type == 'class_definition':
                symbol = self._extract_class(node, file_path)
                if symbol:
                    symbols.append(symbol)
                    # é€’å½’æå–ç±»ä¸­çš„æ–¹æ³•
                    class_name = symbol.name
                    for child in node.children:
                        _extract_from_node(child, class_name)

            # æå–å¯¼å…¥
            elif node.type in ('import_statement', 'import_from_statement'):
                import_symbols = self._extract_imports(node, file_path)
                symbols.extend(import_symbols)

            # æå–å˜é‡ï¼ˆæ¨¡å—çº§åˆ«ï¼‰
            elif node.type == 'assignment' and not parent_name:
                var_symbols = self._extract_variables(node, file_path)
                symbols.extend(var_symbols)

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            else:
                for child in node.children:
                    _extract_from_node(child, parent_name)

        _extract_from_node(tree.root_node)
        return symbols

    def _extract_function(self, node: Node, file_path: str, parent_name: Optional[str]) -> Optional[Symbol]:
        """æå–å‡½æ•°/æ–¹æ³•"""
        # è·å–å‡½æ•°å
        name_node = node.child_by_field_name('name')
        if not name_node:
            return None

        name = self._get_node_text(name_node)

        # åˆ¤æ–­æ˜¯å‡½æ•°è¿˜æ˜¯æ–¹æ³•
        kind = SymbolKind.METHOD if parent_name else SymbolKind.FUNCTION

        # æå–å‚æ•°
        parameters = self._extract_parameters(node)

        # æå–è¿”å›ç±»å‹
        return_type = self._extract_return_type(node)

        # æå–æ–‡æ¡£å­—ç¬¦ä¸²
        docstring = self._extract_docstring(node)

        # ç”Ÿæˆç­¾å
        signature = self._generate_signature(name, parameters, return_type)

        return Symbol(
            name=name,
            kind=kind,
            file_path=file_path,
            start_line=node.start_point[0],
            end_line=node.end_point[0],
            start_byte=node.start_byte,
            end_byte=node.end_byte,
            docstring=docstring,
            signature=signature,
            parent=parent_name,
            parameters=parameters,
            return_type=return_type,
        )

    def _extract_class(self, node: Node, file_path: str) -> Optional[Symbol]:
        """æå–ç±»å®šä¹‰"""
        name_node = node.child_by_field_name('name')
        if not name_node:
            return None

        name = self._get_node_text(name_node)
        docstring = self._extract_docstring(node)

        # æå–åŸºç±»
        bases = self._extract_base_classes(node)

        return Symbol(
            name=name,
            kind=SymbolKind.CLASS,
            file_path=file_path,
            start_line=node.start_point[0],
            end_line=node.end_point[0],
            start_byte=node.start_byte,
            end_byte=node.end_byte,
            docstring=docstring,
            signature=f"class {name}({', '.join(bases)})" if bases else f"class {name}",
        )

    def _extract_imports(self, node: Node, file_path: str) -> List[Symbol]:
        """æå–å¯¼å…¥è¯­å¥"""
        symbols = []

        if node.type == 'import_statement':
            # import module
            for child in node.children:
                if child.type == 'dotted_name':
                    name = self._get_node_text(child)
                    symbols.append(Symbol(
                        name=name,
                        kind=SymbolKind.IMPORT,
                        file_path=file_path,
                        start_line=node.start_point[0],
                        end_line=node.end_point[0],
                        start_byte=node.start_byte,
                        end_byte=node.end_byte,
                    ))

        elif node.type == 'import_from_statement':
            # from module import name
            module_node = node.child_by_field_name('module_name')
            if module_node:
                module_name = self._get_node_text(module_node)
                # æå–å¯¼å…¥çš„åç§°
                for child in node.children:
                    if child.type == 'dotted_name' and child != module_node:
                        name = self._get_node_text(child)
                        symbols.append(Symbol(
                            name=f"{module_name}.{name}",
                            kind=SymbolKind.IMPORT,
                            file_path=file_path,
                            start_line=node.start_point[0],
                            end_line=node.end_point[0],
                            start_byte=node.start_byte,
                            end_byte=node.end_byte,
                        ))

        return symbols

    def _extract_variables(self, node: Node, file_path: str) -> List[Symbol]:
        """æå–å˜é‡å®šä¹‰"""
        symbols = []

        # è·å–èµ‹å€¼å·¦ä¾§
        left = node.child_by_field_name('left')
        if not left:
            return symbols

        # å¤„ç†ç®€å•å˜é‡
        if left.type == 'identifier':
            name = self._get_node_text(left)
            # åˆ¤æ–­æ˜¯å¦æ˜¯å¸¸é‡ï¼ˆå…¨å¤§å†™ï¼‰
            kind = SymbolKind.CONSTANT if name.isupper() else SymbolKind.VARIABLE

            symbols.append(Symbol(
                name=name,
                kind=kind,
                file_path=file_path,
                start_line=node.start_point[0],
                end_line=node.end_point[0],
                start_byte=node.start_byte,
                end_byte=node.end_byte,
            ))

        return symbols

    def _extract_parameters(self, func_node: Node) -> List[Dict[str, Any]]:
        """æå–å‡½æ•°å‚æ•°"""
        parameters = []
        params_node = func_node.child_by_field_name('parameters')

        if params_node:
            for child in params_node.children:
                if child.type == 'identifier':
                    parameters.append({
                        'name': self._get_node_text(child),
                        'type': None,
                        'default': None,
                    })
                elif child.type == 'typed_parameter':
                    name_node = child.child_by_field_name('name')
                    type_node = child.child_by_field_name('type')
                    parameters.append({
                        'name': self._get_node_text(name_node) if name_node else '',
                        'type': self._get_node_text(type_node) if type_node else None,
                        'default': None,
                    })
                elif child.type == 'default_parameter':
                    name_node = child.child_by_field_name('name')
                    value_node = child.child_by_field_name('value')
                    parameters.append({
                        'name': self._get_node_text(name_node) if name_node else '',
                        'type': None,
                        'default': self._get_node_text(value_node) if value_node else None,
                    })

        return parameters

    def _extract_return_type(self, func_node: Node) -> Optional[str]:
        """æå–è¿”å›ç±»å‹"""
        return_type_node = func_node.child_by_field_name('return_type')
        if return_type_node:
            return self._get_node_text(return_type_node)
        return None

    def _extract_docstring(self, node: Node) -> Optional[str]:
        """æå–æ–‡æ¡£å­—ç¬¦ä¸²"""
        # æŸ¥æ‰¾å‡½æ•°/ç±»ä½“ä¸­çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²
        body = node.child_by_field_name('body')
        if body and len(body.children) > 0:
            first_stmt = body.children[0]
            if first_stmt.type == 'expression_statement':
                expr = first_stmt.children[0]
                if expr.type == 'string':
                    docstring = self._get_node_text(expr)
                    # ç§»é™¤å¼•å·
                    return docstring.strip('"""').strip("'''").strip('"').strip("'").strip()
        return None

    def _extract_base_classes(self, class_node: Node) -> List[str]:
        """æå–åŸºç±»"""
        bases = []
        superclasses = class_node.child_by_field_name('superclasses')
        if superclasses:
            for child in superclasses.children:
                if child.type in ('identifier', 'attribute'):
                    bases.append(self._get_node_text(child))
        return bases

    def _generate_signature(self, name: str, parameters: List[Dict], return_type: Optional[str]) -> str:
        """ç”Ÿæˆå‡½æ•°ç­¾å"""
        param_strs = []
        for param in parameters:
            param_str = param['name']
            if param['type']:
                param_str += f": {param['type']}"
            if param['default']:
                param_str += f" = {param['default']}"
            param_strs.append(param_str)

        signature = f"def {name}({', '.join(param_strs)})"
        if return_type:
            signature += f" -> {return_type}"

        return signature

    def _get_node_text(self, node: Node) -> str:
        """è·å–èŠ‚ç‚¹æ–‡æœ¬"""
        return self.source_code[node.start_byte:node.end_byte].decode('utf-8')
```



### 3. JavaScript/TypeScript ç¬¦å·æå–å™¨

```python
# vlinders_server/code_analysis/extractors/javascript_extractor.py

from typing import List, Optional
from tree_sitter import Node, Tree
from ..symbols import Symbol, SymbolKind

class JavaScriptSymbolExtractor:
    """JavaScript/TypeScript ç¬¦å·æå–å™¨"""

    def __init__(self, source_code: bytes):
        self.source_code = source_code

    def extract(self, tree: Tree, file_path: str) -> List[Symbol]:
        """æå–æ‰€æœ‰ç¬¦å·"""
        symbols = []

        def _extract_from_node(node: Node, parent_name: Optional[str] = None):
            # å‡½æ•°å£°æ˜
            if node.type == 'function_declaration':
                symbol = self._extract_function(node, file_path, parent_name)
                if symbol:
                    symbols.append(symbol)

            # ç®­å¤´å‡½æ•°å’Œå‡½æ•°è¡¨è¾¾å¼
            elif node.type in ('arrow_function', 'function_expression'):
                # æŸ¥æ‰¾å˜é‡å£°æ˜
                if node.parent and node.parent.type == 'variable_declarator':
                    symbol = self._extract_function_expression(node.parent, file_path)
                    if symbol:
                        symbols.append(symbol)

            # ç±»å£°æ˜
            elif node.type == 'class_declaration':
                symbol = self._extract_class(node, file_path)
                if symbol:
                    symbols.append(symbol)
                    class_name = symbol.name
                    # æå–ç±»æˆå‘˜
                    for child in node.children:
                        _extract_from_node(child, class_name)

            # æ–¹æ³•å®šä¹‰
            elif node.type == 'method_definition' and parent_name:
                symbol = self._extract_method(node, file_path, parent_name)
                if symbol:
                    symbols.append(symbol)

            # å¯¼å…¥
            elif node.type == 'import_statement':
                import_symbols = self._extract_imports(node, file_path)
                symbols.extend(import_symbols)

            # æ¥å£ï¼ˆTypeScriptï¼‰
            elif node.type == 'interface_declaration':
                symbol = self._extract_interface(node, file_path)
                if symbol:
                    symbols.append(symbol)

            # ç±»å‹åˆ«åï¼ˆTypeScriptï¼‰
            elif node.type == 'type_alias_declaration':
                symbol = self._extract_type_alias(node, file_path)
                if symbol:
                    symbols.append(symbol)

            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            else:
                for child in node.children:
                    _extract_from_node(child, parent_name)

        _extract_from_node(tree.root_node)
        return symbols

    def _extract_function(self, node: Node, file_path: str, parent_name: Optional[str]) -> Optional[Symbol]:
        """æå–å‡½æ•°å£°æ˜"""
        name_node = node.child_by_field_name('name')
        if not name_node:
            return None

        name = self._get_node_text(name_node)
        parameters = self._extract_parameters(node)
        return_type = self._extract_return_type(node)

        signature = self._generate_signature(name, parameters, return_type, is_async=self._is_async(node))

        return Symbol(
            name=name,
            kind=SymbolKind.FUNCTION,
            file_path=file_path,
            start_line=node.start_point[0],
            end_line=node.end_point[0],
            start_byte=node.start_byte,
            end_byte=node.end_byte,
            signature=signature,
            parent=parent_name,
            parameters=parameters,
            return_type=return_type,
        )

    def _extract_class(self, node: Node, file_path: str) -> Optional[Symbol]:
        """æå–ç±»å£°æ˜"""
        name_node = node.child_by_field_name('name')
        if not name_node:
            return None

        name = self._get_node_text(name_node)

        return Symbol(
            name=name,
            kind=SymbolKind.CLASS,
            file_path=file_path,
            start_line=node.start_point[0],
            end_line=node.end_point[0],
            start_byte=node.start_byte,
            end_byte=node.end_byte,
            signature=f"class {name}",
        )

    def _extract_method(self, node: Node, file_path: str, parent_name: str) -> Optional[Symbol]:
        """æå–æ–¹æ³•å®šä¹‰"""
        name_node = node.child_by_field_name('name')
        if not name_node:
            return None

        name = self._get_node_text(name_node)
        parameters = self._extract_parameters(node)
        return_type = self._extract_return_type(node)

        # æ£€æŸ¥ä¿®é¥°ç¬¦
        modifiers = []
        for child in node.children:
            if child.type in ('public', 'private', 'protected', 'static', 'async'):
                modifiers.append(child.type)

        signature = self._generate_signature(name, parameters, return_type, is_async='async' in modifiers)

        return Symbol(
            name=name,
            kind=SymbolKind.METHOD,
            file_path=file_path,
            start_line=node.start_point[0],
            end_line=node.end_point[0],
            start_byte=node.start_byte,
            end_byte=node.end_byte,
            signature=signature,
            parent=parent_name,
            modifiers=modifiers,
            parameters=parameters,
            return_type=return_type,
        )

    def _extract_imports(self, node: Node, file_path: str) -> List[Symbol]:
        """æå–å¯¼å…¥è¯­å¥"""
        symbols = []

        # import { name } from 'module'
        import_clause = node.child_by_field_name('import_clause')
        source = node.child_by_field_name('source')

        if import_clause and source:
            module_name = self._get_node_text(source).strip('"').strip("'")

            # æå–å¯¼å…¥çš„åç§°
            for child in import_clause.children:
                if child.type == 'identifier':
                    name = self._get_node_text(child)
                    symbols.append(Symbol(
                        name=f"{module_name}.{name}",
                        kind=SymbolKind.IMPORT,
                        file_path=file_path,
                        start_line=node.start_point[0],
                        end_line=node.end_point[0],
                        start_byte=node.start_byte,
                        end_byte=node.end_byte,
                    ))

        return symbols

    def _extract_interface(self, node: Node, file_path: str) -> Optional[Symbol]:
        """æå–æ¥å£å£°æ˜ï¼ˆTypeScriptï¼‰"""
        name_node = node.child_by_field_name('name')
        if not name_node:
            return None

        name = self._get_node_text(name_node)

        return Symbol(
            name=name,
            kind=SymbolKind.INTERFACE,
            file_path=file_path,
            start_line=node.start_point[0],
            end_line=node.end_point[0],
            start_byte=node.start_byte,
            end_byte=node.end_byte,
            signature=f"interface {name}",
        )

    def _extract_type_alias(self, node: Node, file_path: str) -> Optional[Symbol]:
        """æå–ç±»å‹åˆ«åï¼ˆTypeScriptï¼‰"""
        name_node = node.child_by_field_name('name')
        if not name_node:
            return None

        name = self._get_node_text(name_node)
        value_node = node.child_by_field_name('value')
        value = self._get_node_text(value_node) if value_node else ''

        return Symbol(
            name=name,
            kind=SymbolKind.TYPE_ALIAS,
            file_path=file_path,
            start_line=node.start_point[0],
            end_line=node.end_point[0],
            start_byte=node.start_byte,
            end_byte=node.end_byte,
            signature=f"type {name} = {value}",
        )

    def _extract_parameters(self, node: Node) -> List[Dict[str, Any]]:
        """æå–å‚æ•°"""
        parameters = []
        params_node = node.child_by_field_name('parameters')

        if params_node:
            for child in params_node.children:
                if child.type == 'identifier':
                    parameters.append({
                        'name': self._get_node_text(child),
                        'type': None,
                    })
                elif child.type == 'required_parameter':
                    pattern = child.child_by_field_name('pattern')
                    type_node = child.child_by_field_name('type')
                    parameters.append({
                        'name': self._get_node_text(pattern) if pattern else '',
                        'type': self._get_node_text(type_node) if type_node else None,
                    })

        return parameters

    def _extract_return_type(self, node: Node) -> Optional[str]:
        """æå–è¿”å›ç±»å‹"""
        return_type_node = node.child_by_field_name('return_type')
        if return_type_node:
            return self._get_node_text(return_type_node)
        return None

    def _is_async(self, node: Node) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚æ­¥å‡½æ•°"""
        for child in node.children:
            if child.type == 'async':
                return True
        return False

    def _generate_signature(self, name: str, parameters: List[Dict], return_type: Optional[str], is_async: bool = False) -> str:
        """ç”Ÿæˆå‡½æ•°ç­¾å"""
        param_strs = []
        for param in parameters:
            param_str = param['name']
            if param['type']:
                param_str += f": {param['type']}"
            param_strs.append(param_str)

        prefix = 'async ' if is_async else ''
        signature = f"{prefix}function {name}({', '.join(param_strs)})"
        if return_type:
            signature += f": {return_type}"

        return signature

    def _get_node_text(self, node: Node) -> str:
        """è·å–èŠ‚ç‚¹æ–‡æœ¬"""
        return self.source_code[node.start_byte:node.end_byte].decode('utf-8')
```

---

## ğŸ”— ä¾èµ–åˆ†æ

### 1. æ¨¡å—ä¾èµ–åˆ†æ

```python
# vlinders_server/code_analysis/dependency_analyzer.py

from typing import Dict, List, Set, Optional
from pathlib import Path
from dataclasses import dataclass, field
from .symbols import Symbol, SymbolKind

@dataclass
class ModuleDependency:
    """æ¨¡å—ä¾èµ–"""
    source_file: str
    target_module: str
    import_type: str  # 'import' | 'from_import'
    imported_names: List[str] = field(default_factory=list)
    line_number: int = 0

@dataclass
class DependencyGraph:
    """ä¾èµ–å›¾"""
    # file_path -> [dependencies]
    dependencies: Dict[str, List[ModuleDependency]] = field(default_factory=dict)
    # file_path -> [files that depend on it]
    reverse_dependencies: Dict[str, Set[str]] = field(default_factory=dict)

class DependencyAnalyzer:
    """ä¾èµ–åˆ†æå™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.dependency_graph = DependencyGraph()

    def analyze_file(self, file_path: Path, symbols: List[Symbol]):
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„ä¾èµ–"""
        file_path_str = str(file_path)
        dependencies = []

        # æå–å¯¼å…¥ç¬¦å·
        for symbol in symbols:
            if symbol.kind == SymbolKind.IMPORT:
                dep = self._parse_import(symbol, file_path)
                if dep:
                    dependencies.append(dep)

        # æ›´æ–°ä¾èµ–å›¾
        self.dependency_graph.dependencies[file_path_str] = dependencies

        # æ›´æ–°åå‘ä¾èµ–
        for dep in dependencies:
            target = dep.target_module
            if target not in self.dependency_graph.reverse_dependencies:
                self.dependency_graph.reverse_dependencies[target] = set()
            self.dependency_graph.reverse_dependencies[target].add(file_path_str)

    def _parse_import(self, symbol: Symbol, file_path: Path) -> Optional[ModuleDependency]:
        """è§£æå¯¼å…¥è¯­å¥"""
        # ä»ç¬¦å·åç§°ä¸­æå–æ¨¡å—å’Œå¯¼å…¥åç§°
        # ä¾‹å¦‚: "os.path" -> module="os", name="path"
        parts = symbol.name.split('.')

        if len(parts) == 1:
            # import module
            return ModuleDependency(
                source_file=str(file_path),
                target_module=parts[0],
                import_type='import',
                line_number=symbol.start_line,
            )
        else:
            # from module import name
            return ModuleDependency(
                source_file=str(file_path),
                target_module='.'.join(parts[:-1]),
                import_type='from_import',
                imported_names=[parts[-1]],
                line_number=symbol.start_line,
            )

    def get_dependencies(self, file_path: str) -> List[ModuleDependency]:
        """è·å–æ–‡ä»¶çš„ä¾èµ–"""
        return self.dependency_graph.dependencies.get(file_path, [])

    def get_dependents(self, file_path: str) -> Set[str]:
        """è·å–ä¾èµ–è¯¥æ–‡ä»¶çš„å…¶ä»–æ–‡ä»¶"""
        return self.dependency_graph.reverse_dependencies.get(file_path, set())

    def find_circular_dependencies(self) -> List[List[str]]:
        """æŸ¥æ‰¾å¾ªç¯ä¾èµ–"""
        cycles = []
        visited = set()
        rec_stack = set()

        def _dfs(file_path: str, path: List[str]):
            visited.add(file_path)
            rec_stack.add(file_path)
            path.append(file_path)

            # è®¿é—®ä¾èµ–
            for dep in self.get_dependencies(file_path):
                target = dep.target_module
                if target not in visited:
                    _dfs(target, path.copy())
                elif target in rec_stack:
                    # æ‰¾åˆ°å¾ªç¯
                    cycle_start = path.index(target)
                    cycles.append(path[cycle_start:] + [target])

            rec_stack.remove(file_path)

        for file_path in self.dependency_graph.dependencies.keys():
            if file_path not in visited:
                _dfs(file_path, [])

        return cycles

    def get_dependency_tree(self, file_path: str, max_depth: int = 3) -> Dict:
        """è·å–ä¾èµ–æ ‘"""
        def _build_tree(path: str, depth: int, visited: Set[str]) -> Dict:
            if depth >= max_depth or path in visited:
                return {'file': path, 'dependencies': []}

            visited.add(path)
            deps = self.get_dependencies(path)

            return {
                'file': path,
                'dependencies': [
                    _build_tree(dep.target_module, depth + 1, visited.copy())
                    for dep in deps
                ]
            }

        return _build_tree(file_path, 0, set())
```

### 2. è°ƒç”¨å›¾åˆ†æ

```python
# vlinders_server/code_analysis/call_graph.py

from typing import Dict, List, Set, Optional
from dataclasses import dataclass, field
from tree_sitter import Node, Tree

@dataclass
class FunctionCall:
    """å‡½æ•°è°ƒç”¨"""
    caller: str  # è°ƒç”¨è€…å‡½æ•°å
    callee: str  # è¢«è°ƒç”¨å‡½æ•°å
    file_path: str
    line_number: int

@dataclass
class CallGraph:
    """è°ƒç”¨å›¾"""
    # function_name -> [calls it makes]
    calls: Dict[str, List[FunctionCall]] = field(default_factory=dict)
    # function_name -> [functions that call it]
    callers: Dict[str, Set[str]] = field(default_factory=dict)

class CallGraphAnalyzer:
    """è°ƒç”¨å›¾åˆ†æå™¨"""

    def __init__(self):
        self.call_graph = CallGraph()

    def analyze_python(self, tree: Tree, source_code: bytes, file_path: str):
        """åˆ†æ Python ä»£ç çš„è°ƒç”¨å›¾"""
        current_function = None

        def _analyze_node(node: Node):
            nonlocal current_function

            # è¿›å…¥å‡½æ•°å®šä¹‰
            if node.type == 'function_definition':
                name_node = node.child_by_field_name('name')
                if name_node:
                    old_function = current_function
                    current_function = self._get_node_text(name_node, source_code)

                    # åˆ†æå‡½æ•°ä½“
                    body = node.child_by_field_name('body')
                    if body:
                        for child in body.children:
                            _analyze_node(child)

                    current_function = old_function
                return

            # å‡½æ•°è°ƒç”¨
            if node.type == 'call':
                if current_function:
                    function_node = node.child_by_field_name('function')
                    if function_node:
                        callee = self._get_node_text(function_node, source_code)
                        call = FunctionCall(
                            caller=current_function,
                            callee=callee,
                            file_path=file_path,
                            line_number=node.start_point[0],
                        )

                        # æ›´æ–°è°ƒç”¨å›¾
                        if current_function not in self.call_graph.calls:
                            self.call_graph.calls[current_function] = []
                        self.call_graph.calls[current_function].append(call)

                        # æ›´æ–°åå‘è°ƒç”¨å›¾
                        if callee not in self.call_graph.callers:
                            self.call_graph.callers[callee] = set()
                        self.call_graph.callers[callee].add(current_function)

            # é€’å½’åˆ†æå­èŠ‚ç‚¹
            for child in node.children:
                _analyze_node(child)

        _analyze_node(tree.root_node)

    def get_calls(self, function_name: str) -> List[FunctionCall]:
        """è·å–å‡½æ•°è°ƒç”¨çš„å…¶ä»–å‡½æ•°"""
        return self.call_graph.calls.get(function_name, [])

    def get_callers(self, function_name: str) -> Set[str]:
        """è·å–è°ƒç”¨è¯¥å‡½æ•°çš„å…¶ä»–å‡½æ•°"""
        return self.call_graph.callers.get(function_name, set())

    def find_call_chain(self, start: str, end: str, max_depth: int = 5) -> List[List[str]]:
        """æŸ¥æ‰¾è°ƒç”¨é“¾"""
        chains = []

        def _dfs(current: str, target: str, path: List[str], depth: int):
            if depth > max_depth:
                return

            if current == target:
                chains.append(path + [current])
                return

            path.append(current)
            for call in self.get_calls(current):
                _dfs(call.callee, target, path.copy(), depth + 1)

        _dfs(start, end, [], 0)
        return chains

    def _get_node_text(self, node: Node, source_code: bytes) -> str:
        """è·å–èŠ‚ç‚¹æ–‡æœ¬"""
        return source_code[node.start_byte:node.end_byte].decode('utf-8')
```

---

## ğŸ§® ä»£ç åµŒå…¥

### 1. åµŒå…¥ç”Ÿæˆå™¨

```python
# vlinders_server/code_analysis/embedding_generator.py

from typing import List, Dict, Any, Optional
from pathlib import Path
import torch
from sentence_transformers import SentenceTransformer
from .symbols import Symbol

class EmbeddingGenerator:
    """ä»£ç åµŒå…¥ç”Ÿæˆå™¨"""

    def __init__(self, model_name: str = "microsoft/codebert-base"):
        """
        åˆå§‹åŒ–åµŒå…¥ç”Ÿæˆå™¨

        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
                - microsoft/codebert-base: é€šç”¨ä»£ç åµŒå…¥
                - microsoft/graphcodebert-base: æ”¯æŒæ•°æ®æµ
                - Salesforce/codet5-base: æ”¯æŒå¤šä»»åŠ¡
        """
        self.model = SentenceTransformer(model_name)
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model.to(self.device)

    def generate_symbol_embedding(self, symbol: Symbol, source_code: str) -> List[float]:
        """ä¸ºç¬¦å·ç”ŸæˆåµŒå…¥"""
        # æ„å»ºç¬¦å·çš„æ–‡æœ¬è¡¨ç¤º
        text = self._build_symbol_text(symbol, source_code)

        # ç”ŸæˆåµŒå…¥
        embedding = self.model.encode(text, convert_to_tensor=True)

        return embedding.cpu().tolist()

    def generate_code_embedding(self, code: str, language: str) -> List[float]:
        """ä¸ºä»£ç ç‰‡æ®µç”ŸæˆåµŒå…¥"""
        # æ·»åŠ è¯­è¨€æ ‡è®°
        text = f"[{language}] {code}"

        embedding = self.model.encode(text, convert_to_tensor=True)
        return embedding.cpu().tolist()

    def generate_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥"""
        embeddings = self.model.encode(
            texts,
            convert_to_tensor=True,
            batch_size=32,
            show_progress_bar=True
        )

        return embeddings.cpu().tolist()

    def _build_symbol_text(self, symbol: Symbol, source_code: str) -> str:
        """æ„å»ºç¬¦å·çš„æ–‡æœ¬è¡¨ç¤º"""
        parts = []

        # æ·»åŠ ç¬¦å·ç±»å‹
        parts.append(f"[{symbol.kind.value}]")

        # æ·»åŠ ç­¾å
        if symbol.signature:
            parts.append(symbol.signature)

        # æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²
        if symbol.docstring:
            parts.append(symbol.docstring)

        # æ·»åŠ ä»£ç å†…å®¹ï¼ˆæˆªå–ï¼‰
        code_lines = source_code.split('\n')[symbol.start_line:symbol.end_line + 1]
        code = '\n'.join(code_lines)
        if len(code) > 500:
            code = code[:500] + '...'
        parts.append(code)

        return '\n'.join(parts)
```

### 2. ä»£ç åˆ†å—ç­–ç•¥

```python
# vlinders_server/code_analysis/code_chunker.py

from typing import List, Dict, Any
from dataclasses import dataclass
from .symbols import Symbol, SymbolKind

@dataclass
class CodeChunk:
    """ä»£ç å—"""
    content: str
    file_path: str
    start_line: int
    end_line: int
    chunk_type: str  # 'function' | 'class' | 'file'
    metadata: Dict[str, Any]

class CodeChunker:
    """ä»£ç åˆ†å—å™¨"""

    def __init__(self, max_chunk_size: int = 1000):
        self.max_chunk_size = max_chunk_size

    def chunk_by_symbols(self, source_code: str, symbols: List[Symbol], file_path: str) -> List[CodeChunk]:
        """åŸºäºç¬¦å·åˆ†å—"""
        chunks = []
        lines = source_code.split('\n')

        for symbol in symbols:
            # åªä¸ºå‡½æ•°å’Œç±»åˆ›å»ºå—
            if symbol.kind not in (SymbolKind.FUNCTION, SymbolKind.METHOD, SymbolKind.CLASS):
                continue

            # æå–ä»£ç 
            code_lines = lines[symbol.start_line:symbol.end_line + 1]
            content = '\n'.join(code_lines)

            # å¦‚æœå¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å—
            if len(content) > self.max_chunk_size:
                sub_chunks = self._split_large_chunk(content, symbol, file_path)
                chunks.extend(sub_chunks)
            else:
                chunks.append(CodeChunk(
                    content=content,
                    file_path=file_path,
                    start_line=symbol.start_line,
                    end_line=symbol.end_line,
                    chunk_type=symbol.kind.value,
                    metadata={
                        'symbol_name': symbol.name,
                        'signature': symbol.signature,
                        'docstring': symbol.docstring,
                    }
                ))

        return chunks

    def chunk_by_size(self, source_code: str, file_path: str) -> List[CodeChunk]:
        """åŸºäºå¤§å°åˆ†å—ï¼ˆç”¨äºæ²¡æœ‰ç¬¦å·çš„æ–‡ä»¶ï¼‰"""
        chunks = []
        lines = source_code.split('\n')

        chunk_lines = []
        chunk_start = 0

        for i, line in enumerate(lines):
            chunk_lines.append(line)

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å¤§å°
            if len('\n'.join(chunk_lines)) >= self.max_chunk_size:
                chunks.append(CodeChunk(
                    content='\n'.join(chunk_lines),
                    file_path=file_path,
                    start_line=chunk_start,
                    end_line=i,
                    chunk_type='file',
                    metadata={}
                ))

                chunk_lines = []
                chunk_start = i + 1

        # æ·»åŠ æœ€åä¸€å—
        if chunk_lines:
            chunks.append(CodeChunk(
                content='\n'.join(chunk_lines),
                file_path=file_path,
                start_line=chunk_start,
                end_line=len(lines) - 1,
                chunk_type='file',
                metadata={}
            ))

        return chunks

    def _split_large_chunk(self, content: str, symbol: Symbol, file_path: str) -> List[CodeChunk]:
        """åˆ†å‰²å¤§å—"""
        chunks = []
        lines = content.split('\n')

        chunk_lines = []
        chunk_start = symbol.start_line

        for i, line in enumerate(lines):
            chunk_lines.append(line)

            if len('\n'.join(chunk_lines)) >= self.max_chunk_size:
                chunks.append(CodeChunk(
                    content='\n'.join(chunk_lines),
                    file_path=file_path,
                    start_line=chunk_start,
                    end_line=symbol.start_line + i,
                    chunk_type=f"{symbol.kind.value}_part",
                    metadata={
                        'symbol_name': symbol.name,
                        'part': len(chunks) + 1,
                    }
                ))

                chunk_lines = []
                chunk_start = symbol.start_line + i + 1

        if chunk_lines:
            chunks.append(CodeChunk(
                content='\n'.join(chunk_lines),
                file_path=file_path,
                start_line=chunk_start,
                end_line=symbol.end_line,
                chunk_type=f"{symbol.kind.value}_part",
                metadata={
                    'symbol_name': symbol.name,
                    'part': len(chunks) + 1,
                }
            ))

        return chunks
```



---

## ğŸ” è¯­ä¹‰æœç´¢

### 1. Qdrant é›†æˆ

```python
# vlinders_server/code_analysis/vector_store.py

from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance,
    VectorParams,
    PointStruct,
    Filter,
    FieldCondition,
    MatchValue,
    MatchText,
)
from .code_chunker import CodeChunk

class CodeVectorStore:
    """ä»£ç å‘é‡å­˜å‚¨"""

    def __init__(self, host: str = "localhost", port: int = 6333):
        self.client = QdrantClient(host=host, port=port)
        self.collection_name = "code_embeddings"
        self.vector_size = 768  # CodeBERT åµŒå…¥ç»´åº¦

    def initialize(self):
        """åˆå§‹åŒ–é›†åˆ"""
        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        collections = self.client.get_collections().collections
        collection_names = [c.name for c in collections]

        if self.collection_name not in collection_names:
            # åˆ›å»ºé›†åˆ
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.vector_size,
                    distance=Distance.COSINE
                )
            )

            # åˆ›å»º payload ç´¢å¼•
            self._create_indexes()

    def _create_indexes(self):
        """åˆ›å»º payload ç´¢å¼•"""
        # ä¸ºå¸¸ç”¨å­—æ®µåˆ›å»ºç´¢å¼•ï¼ŒåŠ é€Ÿè¿‡æ»¤
        self.client.create_payload_index(
            collection_name=self.collection_name,
            field_name="language",
            field_schema="keyword"
        )

        self.client.create_payload_index(
            collection_name=self.collection_name,
            field_name="file_path",
            field_schema="keyword"
        )

        self.client.create_payload_index(
            collection_name=self.collection_name,
            field_name="chunk_type",
            field_schema="keyword"
        )

        self.client.create_payload_index(
            collection_name=self.collection_name,
            field_name="symbol_name",
            field_schema="text"
        )

    def add_chunks(self, chunks: List[CodeChunk], embeddings: List[List[float]], language: str):
        """æ·»åŠ ä»£ç å—"""
        points = []

        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            point = PointStruct(
                id=self._generate_id(chunk),
                vector=embedding,
                payload={
                    'content': chunk.content,
                    'file_path': chunk.file_path,
                    'start_line': chunk.start_line,
                    'end_line': chunk.end_line,
                    'chunk_type': chunk.chunk_type,
                    'language': language,
                    **chunk.metadata
                }
            )
            points.append(point)

        # æ‰¹é‡æ’å…¥
        self.client.upsert(
            collection_name=self.collection_name,
            points=points
        )

    def search(
        self,
        query_embedding: List[float],
        language: Optional[str] = None,
        file_path: Optional[str] = None,
        chunk_type: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢"""
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filter_conditions = []

        if language:
            filter_conditions.append(
                FieldCondition(
                    key="language",
                    match=MatchValue(value=language)
                )
            )

        if file_path:
            filter_conditions.append(
                FieldCondition(
                    key="file_path",
                    match=MatchValue(value=file_path)
                )
            )

        if chunk_type:
            filter_conditions.append(
                FieldCondition(
                    key="chunk_type",
                    match=MatchValue(value=chunk_type)
                )
            )

        # æ‰§è¡Œæœç´¢
        search_result = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            query_filter=Filter(must=filter_conditions) if filter_conditions else None,
            limit=limit
        )

        # æ ¼å¼åŒ–ç»“æœ
        results = []
        for hit in search_result:
            results.append({
                'score': hit.score,
                'content': hit.payload['content'],
                'file_path': hit.payload['file_path'],
                'start_line': hit.payload['start_line'],
                'end_line': hit.payload['end_line'],
                'chunk_type': hit.payload['chunk_type'],
                'metadata': {k: v for k, v in hit.payload.items()
                           if k not in ('content', 'file_path', 'start_line', 'end_line', 'chunk_type', 'language')}
            })

        return results

    def search_by_symbol(self, symbol_name: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æŒ‰ç¬¦å·åç§°æœç´¢"""
        search_result = self.client.scroll(
            collection_name=self.collection_name,
            scroll_filter=Filter(
                must=[
                    FieldCondition(
                        key="symbol_name",
                        match=MatchText(text=symbol_name)
                    )
                ]
            ),
            limit=limit
        )

        results = []
        for point in search_result[0]:
            results.append({
                'content': point.payload['content'],
                'file_path': point.payload['file_path'],
                'start_line': point.payload['start_line'],
                'end_line': point.payload['end_line'],
                'metadata': point.payload.get('metadata', {})
            })

        return results

    def delete_by_file(self, file_path: str):
        """åˆ é™¤æ–‡ä»¶çš„æ‰€æœ‰å—"""
        self.client.delete(
            collection_name=self.collection_name,
            points_selector=Filter(
                must=[
                    FieldCondition(
                        key="file_path",
                        match=MatchValue(value=file_path)
                    )
                ]
            )
        )

    def _generate_id(self, chunk: CodeChunk) -> str:
        """ç”Ÿæˆå”¯ä¸€ ID"""
        import hashlib
        content = f"{chunk.file_path}:{chunk.start_line}:{chunk.end_line}"
        return hashlib.sha256(content.encode()).hexdigest()
```

### 2. æœç´¢ç­–ç•¥

```python
# vlinders_server/code_analysis/search_strategy.py

from typing import List, Dict, Any, Optional
from enum import Enum
from .embedding_generator import EmbeddingGenerator
from .vector_store import CodeVectorStore

class SearchMode(Enum):
    """æœç´¢æ¨¡å¼"""
    SEMANTIC = "semantic"  # çº¯è¯­ä¹‰æœç´¢
    HYBRID = "hybrid"      # æ··åˆæœç´¢ï¼ˆè¯­ä¹‰ + å…³é”®è¯ï¼‰
    SYMBOL = "symbol"      # ç¬¦å·æœç´¢

class CodeSearchEngine:
    """ä»£ç æœç´¢å¼•æ“"""

    def __init__(self, embedding_generator: EmbeddingGenerator, vector_store: CodeVectorStore):
        self.embedding_generator = embedding_generator
        self.vector_store = vector_store

    def search(
        self,
        query: str,
        mode: SearchMode = SearchMode.SEMANTIC,
        language: Optional[str] = None,
        file_path: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """æœç´¢ä»£ç """
        if mode == SearchMode.SYMBOL:
            return self._search_by_symbol(query, limit)
        elif mode == SearchMode.SEMANTIC:
            return self._semantic_search(query, language, file_path, limit)
        elif mode == SearchMode.HYBRID:
            return self._hybrid_search(query, language, file_path, limit)

    def _semantic_search(
        self,
        query: str,
        language: Optional[str],
        file_path: Optional[str],
        limit: int
    ) -> List[Dict[str, Any]]:
        """çº¯è¯­ä¹‰æœç´¢"""
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_generator.generate_code_embedding(
            query,
            language or "unknown"
        )

        # å‘é‡æœç´¢
        results = self.vector_store.search(
            query_embedding=query_embedding,
            language=language,
            file_path=file_path,
            limit=limit
        )

        return results

    def _hybrid_search(
        self,
        query: str,
        language: Optional[str],
        file_path: Optional[str],
        limit: int
    ) -> List[Dict[str, Any]]:
        """æ··åˆæœç´¢"""
        # 1. è¯­ä¹‰æœç´¢
        semantic_results = self._semantic_search(query, language, file_path, limit * 2)

        # 2. å…³é”®è¯è¿‡æ»¤
        keywords = self._extract_keywords(query)
        filtered_results = []

        for result in semantic_results:
            content = result['content'].lower()
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_score = sum(1 for kw in keywords if kw.lower() in content) / len(keywords)

            # ç»„åˆåˆ†æ•°
            result['combined_score'] = result['score'] * 0.7 + keyword_score * 0.3
            filtered_results.append(result)

        # æŒ‰ç»„åˆåˆ†æ•°æ’åº
        filtered_results.sort(key=lambda x: x['combined_score'], reverse=True)

        return filtered_results[:limit]

    def _search_by_symbol(self, symbol_name: str, limit: int) -> List[Dict[str, Any]]:
        """ç¬¦å·æœç´¢"""
        return self.vector_store.search_by_symbol(symbol_name, limit)

    def _extract_keywords(self, query: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ NLP æ–¹æ³•ï¼‰
        import re
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        words = re.findall(r'\w+', query)
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}
        keywords = [w for w in words if w.lower() not in stop_words]
        return keywords

    def search_similar_code(
        self,
        code: str,
        language: str,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸ä¼¼ä»£ç """
        # ç”Ÿæˆä»£ç åµŒå…¥
        code_embedding = self.embedding_generator.generate_code_embedding(code, language)

        # æœç´¢
        results = self.vector_store.search(
            query_embedding=code_embedding,
            language=language,
            limit=limit
        )

        return results

    def search_by_functionality(
        self,
        description: str,
        language: Optional[str] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """æŒ‰åŠŸèƒ½æè¿°æœç´¢"""
        # ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°æœç´¢
        return self._semantic_search(description, language, None, limit)
```

---

## ğŸ“Š ä»£ç ç´¢å¼•

### 1. ç´¢å¼•æ„å»ºå™¨

```python
# vlinders_server/code_analysis/indexer.py

from typing import List, Optional
from pathlib import Path
import asyncio
from concurrent.futures import ThreadPoolExecutor
from .parser_manager import ParserManager
from .ast_parser import ASTParser
from .extractors.python_extractor import PythonSymbolExtractor
from .extractors.javascript_extractor import JavaScriptSymbolExtractor
from .dependency_analyzer import DependencyAnalyzer
from .call_graph import CallGraphAnalyzer
from .embedding_generator import EmbeddingGenerator
from .code_chunker import CodeChunker
from .vector_store import CodeVectorStore

class CodeIndexer:
    """ä»£ç ç´¢å¼•å™¨"""

    def __init__(
        self,
        project_root: Path,
        parser_manager: ParserManager,
        embedding_generator: EmbeddingGenerator,
        vector_store: CodeVectorStore
    ):
        self.project_root = project_root
        self.parser_manager = parser_manager
        self.ast_parser = ASTParser(parser_manager)
        self.embedding_generator = embedding_generator
        self.vector_store = vector_store
        self.code_chunker = CodeChunker()

        # åˆ†æå™¨
        self.dependency_analyzer = DependencyAnalyzer(project_root)
        self.call_graph_analyzer = CallGraphAnalyzer()

        # ç¬¦å·æå–å™¨æ˜ å°„
        self.extractors = {
            'python': PythonSymbolExtractor,
            'javascript': JavaScriptSymbolExtractor,
            'typescript': JavaScriptSymbolExtractor,
        }

    async def index_project(self, exclude_patterns: Optional[List[str]] = None):
        """ç´¢å¼•æ•´ä¸ªé¡¹ç›®"""
        # æŸ¥æ‰¾æ‰€æœ‰ä»£ç æ–‡ä»¶
        code_files = self._find_code_files(exclude_patterns or [])

        print(f"Found {len(code_files)} code files")

        # å¹¶è¡Œå¤„ç†æ–‡ä»¶
        with ThreadPoolExecutor(max_workers=8) as executor:
            tasks = [
                asyncio.get_event_loop().run_in_executor(
                    executor,
                    self.index_file,
                    file_path
                )
                for file_path in code_files
            ]

            await asyncio.gather(*tasks)

        print("Indexing complete")

    def index_file(self, file_path: Path):
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        try:
            # æ£€æµ‹è¯­è¨€
            language = self.parser_manager.detect_language(file_path)
            if not language:
                return

            # è§£æ AST
            tree = self.ast_parser.parse_file(file_path)
            if not tree:
                return

            # è¯»å–æºä»£ç 
            source_code = file_path.read_text(encoding='utf-8')
            source_bytes = source_code.encode('utf-8')

            # æå–ç¬¦å·
            symbols = []
            if language in self.extractors:
                extractor_class = self.extractors[language]
                extractor = extractor_class(source_bytes)
                symbols = extractor.extract(tree, str(file_path))

            # ä¾èµ–åˆ†æ
            self.dependency_analyzer.analyze_file(file_path, symbols)

            # è°ƒç”¨å›¾åˆ†æ
            if language == 'python':
                self.call_graph_analyzer.analyze_python(tree, source_bytes, str(file_path))

            # ä»£ç åˆ†å—
            if symbols:
                chunks = self.code_chunker.chunk_by_symbols(source_code, symbols, str(file_path))
            else:
                chunks = self.code_chunker.chunk_by_size(source_code, str(file_path))

            # ç”ŸæˆåµŒå…¥
            chunk_texts = [chunk.content for chunk in chunks]
            embeddings = self.embedding_generator.generate_batch_embeddings(chunk_texts)

            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            self.vector_store.add_chunks(chunks, embeddings, language)

            print(f"Indexed {file_path}: {len(symbols)} symbols, {len(chunks)} chunks")

        except Exception as e:
            print(f"Error indexing {file_path}: {e}")

    def update_file(self, file_path: Path):
        """æ›´æ–°æ–‡ä»¶ç´¢å¼•"""
        # åˆ é™¤æ—§ç´¢å¼•
        self.vector_store.delete_by_file(str(file_path))

        # é‡æ–°ç´¢å¼•
        self.index_file(file_path)

    def delete_file(self, file_path: Path):
        """åˆ é™¤æ–‡ä»¶ç´¢å¼•"""
        self.vector_store.delete_by_file(str(file_path))

    def _find_code_files(self, exclude_patterns: List[str]) -> List[Path]:
        """æŸ¥æ‰¾ä»£ç æ–‡ä»¶"""
        code_extensions = {'.py', '.js', '.jsx', '.ts', '.tsx', '.go', '.rs', '.java', '.c', '.cpp', '.h', '.hpp'}

        code_files = []
        for ext in code_extensions:
            files = self.project_root.rglob(f"*{ext}")
            for file in files:
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤
                if self._should_exclude(file, exclude_patterns):
                    continue
                code_files.append(file)

        return code_files

    def _should_exclude(self, file_path: Path, exclude_patterns: List[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤æ–‡ä»¶"""
        path_str = str(file_path)

        # é»˜è®¤æ’é™¤
        default_excludes = [
            'node_modules',
            '.git',
            '__pycache__',
            '.venv',
            'venv',
            'dist',
            'build',
            '.pytest_cache',
        ]

        for pattern in default_excludes + exclude_patterns:
            if pattern in path_str:
                return True

        return False
```

### 2. å¢é‡ç´¢å¼•

```python
# vlinders_server/code_analysis/incremental_indexer.py

from typing import Set
from pathlib import Path
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent, FileDeletedEvent
from .indexer import CodeIndexer

class CodeFileHandler(FileSystemEventHandler):
    """ä»£ç æ–‡ä»¶å˜æ›´å¤„ç†å™¨"""

    def __init__(self, indexer: CodeIndexer):
        self.indexer = indexer
        self.pending_updates: Set[Path] = set()
        self.last_update_time = {}

    def on_modified(self, event: FileModifiedEvent):
        """æ–‡ä»¶ä¿®æ”¹"""
        if event.is_directory:
            return

        file_path = Path(event.src_path)
        self._schedule_update(file_path)

    def on_created(self, event: FileCreatedEvent):
        """æ–‡ä»¶åˆ›å»º"""
        if event.is_directory:
            return

        file_path = Path(event.src_path)
        self._schedule_update(file_path)

    def on_deleted(self, event: FileDeletedEvent):
        """æ–‡ä»¶åˆ é™¤"""
        if event.is_directory:
            return

        file_path = Path(event.src_path)
        self.indexer.delete_file(file_path)

    def _schedule_update(self, file_path: Path):
        """è°ƒåº¦æ›´æ–°ï¼ˆé˜²æŠ–ï¼‰"""
        current_time = time.time()

        # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç æ–‡ä»¶
        if not self.indexer.parser_manager.detect_language(file_path):
            return

        # é˜²æŠ–ï¼šå¦‚æœæœ€è¿‘åˆšæ›´æ–°è¿‡ï¼Œè·³è¿‡
        if file_path in self.last_update_time:
            if current_time - self.last_update_time[file_path] < 1.0:  # 1 ç§’é˜²æŠ–
                return

        self.pending_updates.add(file_path)
        self.last_update_time[file_path] = current_time

    def process_pending_updates(self):
        """å¤„ç†å¾…æ›´æ–°çš„æ–‡ä»¶"""
        while self.pending_updates:
            file_path = self.pending_updates.pop()
            try:
                self.indexer.update_file(file_path)
            except Exception as e:
                print(f"Error updating {file_path}: {e}")

class IncrementalIndexer:
    """å¢é‡ç´¢å¼•å™¨"""

    def __init__(self, indexer: CodeIndexer):
        self.indexer = indexer
        self.observer = Observer()
        self.handler = CodeFileHandler(indexer)

    def start_watching(self):
        """å¼€å§‹ç›‘å¬æ–‡ä»¶å˜æ›´"""
        self.observer.schedule(
            self.handler,
            str(self.indexer.project_root),
            recursive=True
        )
        self.observer.start()
        print(f"Watching {self.indexer.project_root} for changes...")

    def stop_watching(self):
        """åœæ­¢ç›‘å¬"""
        self.observer.stop()
        self.observer.join()

    def process_updates(self):
        """å¤„ç†å¾…æ›´æ–°çš„æ–‡ä»¶"""
        self.handler.process_pending_updates()
```

---

## ğŸš€ å®Œæ•´ç¤ºä¾‹

### 1. åˆå§‹åŒ–ä»£ç åˆ†æå¼•æ“

```python
# example_usage.py

import asyncio
from pathlib import Path
from vlinders_server.code_analysis.parser_manager import ParserManager
from vlinders_server.code_analysis.embedding_generator import EmbeddingGenerator
from vlinders_server.code_analysis.vector_store import CodeVectorStore
from vlinders_server.code_analysis.indexer import CodeIndexer
from vlinders_server.code_analysis.search_strategy import CodeSearchEngine, SearchMode
from vlinders_server.code_analysis.incremental_indexer import IncrementalIndexer

async def main():
    # 1. åˆå§‹åŒ–ç»„ä»¶
    project_root = Path("/path/to/your/project")
    cache_dir = Path(".cache/tree-sitter")

    parser_manager = ParserManager(cache_dir)
    embedding_generator = EmbeddingGenerator("microsoft/codebert-base")
    vector_store = CodeVectorStore(host="localhost", port=6333)

    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    vector_store.initialize()

    # 2. åˆ›å»ºç´¢å¼•å™¨
    indexer = CodeIndexer(
        project_root=project_root,
        parser_manager=parser_manager,
        embedding_generator=embedding_generator,
        vector_store=vector_store
    )

    # 3. ç´¢å¼•é¡¹ç›®
    print("Indexing project...")
    await indexer.index_project(exclude_patterns=['tests', 'docs'])

    # 4. åˆ›å»ºæœç´¢å¼•æ“
    search_engine = CodeSearchEngine(embedding_generator, vector_store)

    # 5. æœç´¢ç¤ºä¾‹
    print("\n=== Semantic Search ===")
    results = search_engine.search(
        query="function to calculate fibonacci numbers",
        mode=SearchMode.SEMANTIC,
        language="python",
        limit=5
    )

    for i, result in enumerate(results, 1):
        print(f"\n{i}. {result['file_path']}:{result['start_line']}")
        print(f"   Score: {result['score']:.4f}")
        print(f"   Type: {result['chunk_type']}")
        print(f"   Content preview: {result['content'][:100]}...")

    # 6. ç¬¦å·æœç´¢
    print("\n=== Symbol Search ===")
    results = search_engine.search(
        query="fibonacci",
        mode=SearchMode.SYMBOL,
        limit=5
    )

    for i, result in enumerate(results, 1):
        print(f"\n{i}. {result['file_path']}:{result['start_line']}")
        print(f"   Content preview: {result['content'][:100]}...")

    # 7. ç›¸ä¼¼ä»£ç æœç´¢
    print("\n=== Similar Code Search ===")
    code_snippet = """
    def fibonacci(n):
        if n <= 1:
            return n
        return fibonacci(n-1) + fibonacci(n-2)
    """

    results = search_engine.search_similar_code(
        code=code_snippet,
        language="python",
        limit=5
    )

    for i, result in enumerate(results, 1):
        print(f"\n{i}. {result['file_path']}:{result['start_line']}")
        print(f"   Score: {result['score']:.4f}")

    # 8. å¯åŠ¨å¢é‡ç´¢å¼•
    print("\n=== Starting Incremental Indexing ===")
    incremental_indexer = IncrementalIndexer(indexer)
    incremental_indexer.start_watching()

    # ä¿æŒè¿è¡Œ
    try:
        while True:
            await asyncio.sleep(5)
            incremental_indexer.process_updates()
    except KeyboardInterrupt:
        print("\nStopping...")
        incremental_indexer.stop_watching()

if __name__ == "__main__":
    asyncio.run(main())
```

### 2. ä¾èµ–åˆ†æç¤ºä¾‹

```python
# example_dependency_analysis.py

from pathlib import Path
from vlinders_server.code_analysis.parser_manager import ParserManager
from vlinders_server.code_analysis.ast_parser import ASTParser
from vlinders_server.code_analysis.extractors.python_extractor import PythonSymbolExtractor
from vlinders_server.code_analysis.dependency_analyzer import DependencyAnalyzer

def analyze_dependencies(project_root: Path):
    # åˆå§‹åŒ–
    parser_manager = ParserManager(Path(".cache"))
    ast_parser = ASTParser(parser_manager)
    dependency_analyzer = DependencyAnalyzer(project_root)

    # åˆ†ææ‰€æœ‰ Python æ–‡ä»¶
    for py_file in project_root.rglob("*.py"):
        tree = ast_parser.parse_file(py_file)
        if tree:
            source_code = py_file.read_bytes()
            extractor = PythonSymbolExtractor(source_code)
            symbols = extractor.extract(tree, str(py_file))
            dependency_analyzer.analyze_file(py_file, symbols)

    # æŸ¥æ‰¾å¾ªç¯ä¾èµ–
    cycles = dependency_analyzer.find_circular_dependencies()
    if cycles:
        print("Found circular dependencies:")
        for cycle in cycles:
            print(f"  {' -> '.join(cycle)}")
    else:
        print("No circular dependencies found")

    # è·å–ä¾èµ–æ ‘
    main_file = project_root / "main.py"
    if main_file.exists():
        dep_tree = dependency_analyzer.get_dependency_tree(str(main_file))
        print(f"\nDependency tree for {main_file}:")
        print_tree(dep_tree)

def print_tree(tree, indent=0):
    """æ‰“å°ä¾èµ–æ ‘"""
    print("  " * indent + f"- {tree['file']}")
    for dep in tree['dependencies']:
        print_tree(dep, indent + 1)

if __name__ == "__main__":
    analyze_dependencies(Path("/path/to/project"))
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. ç¼“å­˜ç­–ç•¥

- **AST ç¼“å­˜** - ç¼“å­˜è§£æåçš„ ASTï¼Œé¿å…é‡å¤è§£æ
- **åµŒå…¥ç¼“å­˜** - ç¼“å­˜ç”Ÿæˆçš„åµŒå…¥å‘é‡
- **ç¬¦å·ç´¢å¼•** - ä½¿ç”¨ Redis ç¼“å­˜ç¬¦å·ç´¢å¼•

### 2. å¹¶è¡Œå¤„ç†

- **å¤šçº¿ç¨‹è§£æ** - ä½¿ç”¨ ThreadPoolExecutor å¹¶è¡Œè§£ææ–‡ä»¶
- **æ‰¹é‡åµŒå…¥** - æ‰¹é‡ç”ŸæˆåµŒå…¥ï¼Œæé«˜ GPU åˆ©ç”¨ç‡
- **å¼‚æ­¥ I/O** - ä½¿ç”¨ asyncio å¤„ç†æ–‡ä»¶è¯»å†™

### 3. å¢é‡æ›´æ–°

- **æ–‡ä»¶ç›‘å¬** - ä½¿ç”¨ watchdog ç›‘å¬æ–‡ä»¶å˜æ›´
- **å¢é‡è§£æ** - åªé‡æ–°è§£æä¿®æ”¹çš„éƒ¨åˆ†
- **é˜²æŠ–å¤„ç†** - é¿å…é¢‘ç¹æ›´æ–°

---

## ğŸ¯ æ€»ç»“

ä»£ç åˆ†æå¼•æ“æ˜¯ Vlinders-Server çš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›ï¼š

1. **Tree-sitter é›†æˆ** - é«˜æ€§èƒ½ã€å¤šè¯­è¨€çš„è¯­æ³•è§£æ
2. **ç¬¦å·æå–** - æå–å‡½æ•°ã€ç±»ã€å˜é‡ç­‰ä»£ç ç¬¦å·
3. **ä¾èµ–åˆ†æ** - åˆ†ææ¨¡å—ä¾èµ–å’Œè°ƒç”¨å…³ç³»
4. **ä»£ç åµŒå…¥** - ç”Ÿæˆä»£ç å‘é‡ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢
5. **Qdrant é›†æˆ** - é«˜æ€§èƒ½å‘é‡æœç´¢
6. **å¢é‡ç´¢å¼•** - å®æ—¶æ›´æ–°ä»£ç ç´¢å¼•

é€šè¿‡è¿™äº›åŠŸèƒ½ï¼ŒAI Agent å¯ä»¥ï¼š
- å¿«é€ŸæŸ¥æ‰¾ç›¸å…³ä»£ç 
- ç†è§£ä»£ç ç»“æ„å’Œä¾èµ–
- è¿›è¡Œè¯­ä¹‰çº§åˆ«çš„ä»£ç æœç´¢
- è·å–ç²¾å‡†çš„ä»£ç ä¸Šä¸‹æ–‡

---

**ç›¸å…³æ–‡æ¡£**:
- [02-æŠ€æœ¯é€‰å‹.md](./02-æŠ€æœ¯é€‰å‹.md) - Tree-sitter é€‰å‹ç†ç”±
- [05-å·¥å…·æ‰§è¡Œå¼•æ“.md](./05-å·¥å…·æ‰§è¡Œå¼•æ“.md) - ä»£ç åˆ†æå·¥å…·é›†æˆ
- [04-Agentç¼–æ’ç³»ç»Ÿ.md](./04-Agentç¼–æ’ç³»ç»Ÿ.md) - Agent å¦‚ä½•ä½¿ç”¨ä»£ç åˆ†æ
