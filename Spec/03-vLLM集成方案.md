# vLLM é›†æˆæ–¹æ¡ˆ

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-28
**æ–‡æ¡£ç±»å‹**: æŠ€æœ¯å®æ–½

---

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨ Vlinders-Server ä¸­é›†æˆå’Œä½¿ç”¨ vLLMï¼ŒåŒ…æ‹¬å®‰è£…ã€é…ç½®ã€ä¼˜åŒ–å’Œç”Ÿäº§éƒ¨ç½²ã€‚

---

## ğŸ¯ vLLM ç®€ä»‹

### ä»€ä¹ˆæ˜¯ vLLMï¼Ÿ

vLLM æ˜¯ç”± UC Berkeley å¼€å‘çš„é«˜æ€§èƒ½å¤§æ¨¡å‹æ¨ç†å¼•æ“ï¼Œä¸“ä¸ºç”Ÿäº§ç¯å¢ƒè®¾è®¡ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
- **PagedAttention** - é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå†…å­˜åˆ©ç”¨ç‡æå‡ 4x
- **Continuous Batching** - è¿ç»­æ‰¹å¤„ç†ï¼Œååé‡æå‡ 24x
- **å¼‚æ­¥æ¨ç†** - åŸç”Ÿæ”¯æŒ Python asyncio
- **æµå¼ç”Ÿæˆ** - å®æ—¶è¾“å‡º Token
- **å¤š GPU æ”¯æŒ** - Tensor Parallelism

### æ€§èƒ½å¯¹æ¯”

```
åŸºå‡†æµ‹è¯• (Llama 3 70B, 4x A100 80GB):

ä¼ ç»Ÿæ–¹æ³•:
- ååé‡: 10 requests/s
- å»¶è¿Ÿ: 2000ms (TTFT)
- GPU åˆ©ç”¨ç‡: 60%

vLLM:
- ååé‡: 240 requests/s (24x â†‘)
- å»¶è¿Ÿ: 500ms (TTFT) (4x â†“)
- GPU åˆ©ç”¨ç‡: 90%
```

---

## ğŸ”§ å®‰è£…å’Œé…ç½®

### ç¯å¢ƒè¦æ±‚

**ç¡¬ä»¶**:
- GPU: NVIDIA A100/H100 (æ¨è) æˆ– V100/A10
- VRAM: è‡³å°‘ 40GB (Llama 3 70B éœ€è¦ 4x 80GB)
- CPU: 16+ æ ¸å¿ƒ
- RAM: 128GB+
- å­˜å‚¨: 500GB+ SSD

**è½¯ä»¶**:
- OS: Ubuntu 22.04 LTS
- Python: 3.10 æˆ– 3.11
- CUDA: 12.1+
- cuDNN: 8.9+
- Driver: 535+

### å®‰è£…æ­¥éª¤

#### 1. å®‰è£… CUDA å’Œ cuDNN

```bash
# å®‰è£… CUDA 12.1
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run
sudo sh cuda_12.1.0_530.30.02_linux.run

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_HOME=/usr/local/cuda-12.1
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# éªŒè¯å®‰è£…
nvcc --version
nvidia-smi
```

#### 2. åˆ›å»º Python ç¯å¢ƒ

```bash
# ä½¿ç”¨ conda æˆ– venv
conda create -n vlinders python=3.11
conda activate vlinders

# æˆ–ä½¿ç”¨ venv
python3.11 -m venv venv
source venv/bin/activate
```

#### 3. å®‰è£… vLLM

```bash
# ä» PyPI å®‰è£…ï¼ˆæ¨èï¼‰
pip install vllm

# æˆ–ä»æºç å®‰è£…ï¼ˆæœ€æ–°åŠŸèƒ½ï¼‰
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .

# éªŒè¯å®‰è£…
python -c "import vllm; print(vllm.__version__)"
```

#### 4. å®‰è£…ä¾èµ–

```bash
# å®‰è£…å…¶ä»–ä¾èµ–
pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu121
pip install transformers accelerate
pip install fastapi uvicorn
pip install prometheus-client

# éªŒè¯ CUDA å¯ç”¨
python -c "import torch; print(torch.cuda.is_available())"
```

---

## ğŸ“¦ æ¨¡å‹åŠ è½½

### æ”¯æŒçš„æ¨¡å‹æ ¼å¼

vLLM æ”¯æŒä»¥ä¸‹æ¨¡å‹æ ¼å¼ï¼š
- Hugging Face Transformers
- GGUF (é€šè¿‡è½¬æ¢)
- AWQ (é‡åŒ–)
- GPTQ (é‡åŒ–)

### ä¸‹è½½æ¨¡å‹

```bash
# ä½¿ç”¨ Hugging Face CLI
pip install huggingface-hub

# ä¸‹è½½æ¨¡å‹
huggingface-cli download meta-llama/Meta-Llama-3-70B-Instruct \
  --local-dir /models/llama-3-70b \
  --local-dir-use-symlinks False

# æˆ–ä½¿ç”¨ Python
from huggingface_hub import snapshot_download

snapshot_download(
    repo_id="meta-llama/Meta-Llama-3-70B-Instruct",
    local_dir="/models/llama-3-70b",
    local_dir_use_symlinks=False
)
```

### åŠ è½½æ¨¡å‹åˆ° vLLM

```python
from vllm import AsyncLLMEngine
from vllm.engine.arg_utils import AsyncEngineArgs

# é…ç½®å¼•æ“å‚æ•°
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b",
    tensor_parallel_size=4,  # ä½¿ç”¨ 4 ä¸ª GPU
    dtype="float16",         # ä½¿ç”¨ FP16
    max_model_len=32768,     # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    gpu_memory_utilization=0.9,  # GPU å†…å­˜ä½¿ç”¨ç‡
    trust_remote_code=True,
    enable_prefix_caching=True,  # å¯ç”¨å‰ç¼€ç¼“å­˜
    disable_log_stats=False
)

# åˆ›å»ºå¼•æ“
engine = AsyncLLMEngine.from_engine_args(engine_args)

print(f"Model loaded successfully")
print(f"Max tokens: {engine_args.max_model_len}")
print(f"Tensor parallel size: {engine_args.tensor_parallel_size}")
```

### å¤šæ¨¡å‹ç®¡ç†

```python
class ModelManager:
    """ç®¡ç†å¤šä¸ªæ¨¡å‹"""

    def __init__(self):
        self.engines = {}

    async def load_model(
        self,
        model_name: str,
        model_path: str,
        tensor_parallel_size: int = 1
    ):
        """åŠ è½½æ¨¡å‹"""
        engine_args = AsyncEngineArgs(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
            dtype="float16",
            max_model_len=32768,
            gpu_memory_utilization=0.9
        )

        engine = AsyncLLMEngine.from_engine_args(engine_args)
        self.engines[model_name] = engine

        print(f"âœ… Model {model_name} loaded")

    def get_engine(self, model_name: str):
        """è·å–æ¨¡å‹å¼•æ“"""
        if model_name not in self.engines:
            raise ValueError(f"Model {model_name} not loaded")
        return self.engines[model_name]

# ä½¿ç”¨ç¤ºä¾‹
manager = ModelManager()

# åŠ è½½å¤šä¸ªæ¨¡å‹
await manager.load_model(
    "vlinders-gpt-4",
    "/models/llama-3-70b",
    tensor_parallel_size=4
)

await manager.load_model(
    "vlinders-gpt-3.5",
    "/models/llama-3-8b",
    tensor_parallel_size=1
)
```

---

## ğŸš€ æ¨ç†æ¥å£

### å¼‚æ­¥æ¨ç†

```python
from vllm import SamplingParams
import uuid

async def generate_text(
    engine: AsyncLLMEngine,
    prompt: str,
    max_tokens: int = 2048,
    temperature: float = 0.7,
    top_p: float = 0.95,
    stop: list = None
):
    """å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬"""

    # é…ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        stop=stop or []
    )

    # ç”Ÿæˆè¯·æ±‚ ID
    request_id = f"req_{uuid.uuid4().hex[:8]}"

    # å¼‚æ­¥ç”Ÿæˆ
    final_output = None
    async for output in engine.generate(
        prompt,
        sampling_params,
        request_id
    ):
        final_output = output

    # è¿”å›ç»“æœ
    if final_output and final_output.outputs:
        return {
            "text": final_output.outputs[0].text,
            "finish_reason": final_output.outputs[0].finish_reason,
            "usage": {
                "prompt_tokens": len(final_output.prompt_token_ids),
                "completion_tokens": len(final_output.outputs[0].token_ids),
                "total_tokens": len(final_output.prompt_token_ids) + len(final_output.outputs[0].token_ids)
            }
        }

# ä½¿ç”¨ç¤ºä¾‹
result = await generate_text(
    engine,
    "Write a fibonacci function in Python:",
    max_tokens=500,
    temperature=0.2
)

print(result["text"])
print(f"Tokens used: {result['usage']['total_tokens']}")
```

### æµå¼ç”Ÿæˆ

```python
async def generate_stream(
    engine: AsyncLLMEngine,
    prompt: str,
    max_tokens: int = 2048,
    temperature: float = 0.7
):
    """æµå¼ç”Ÿæˆæ–‡æœ¬"""

    sampling_params = SamplingParams(
        temperature=temperature,
        max_tokens=max_tokens
    )

    request_id = f"req_{uuid.uuid4().hex[:8]}"

    # æµå¼ç”Ÿæˆ
    async for output in engine.generate(
        prompt,
        sampling_params,
        request_id
    ):
        if output.outputs:
            yield {
                "text": output.outputs[0].text,
                "finish_reason": output.outputs[0].finish_reason
            }

# ä½¿ç”¨ç¤ºä¾‹
async for chunk in generate_stream(
    engine,
    "Write a story about AI:",
    max_tokens=1000
):
    print(chunk["text"], end="", flush=True)
    if chunk["finish_reason"]:
        print(f"\n[{chunk['finish_reason']}]")
```

### æ‰¹å¤„ç†

```python
async def generate_batch(
    engine: AsyncLLMEngine,
    prompts: list[str],
    max_tokens: int = 2048
):
    """æ‰¹é‡ç”Ÿæˆ"""

    sampling_params = SamplingParams(
        max_tokens=max_tokens,
        temperature=0.7
    )

    # æäº¤æ‰€æœ‰è¯·æ±‚
    request_ids = []
    for prompt in prompts:
        request_id = f"req_{uuid.uuid4().hex[:8]}"
        request_ids.append(request_id)

        # å¼‚æ­¥æäº¤ï¼ˆä¸ç­‰å¾…ï¼‰
        asyncio.create_task(
            engine.generate(prompt, sampling_params, request_id)
        )

    # vLLM ä¼šè‡ªåŠ¨æ‰¹å¤„ç†è¿™äº›è¯·æ±‚
    # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
    results = []
    for request_id in request_ids:
        # è·å–ç»“æœï¼ˆå®é™…å®ç°éœ€è¦ç»“æœé˜Ÿåˆ—ï¼‰
        result = await get_result(request_id)
        results.append(result)

    return results
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. GPU å†…å­˜ç®¡ç†

```python
# é…ç½® GPU å†…å­˜ä½¿ç”¨ç‡
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b",
    gpu_memory_utilization=0.9,  # ä½¿ç”¨ 90% GPU å†…å­˜
    # 0.9 æ˜¯æ¨èå€¼ï¼Œç•™ 10% ç»™ç³»ç»Ÿ
)

# ç›‘æ§ GPU å†…å­˜
import torch

def print_gpu_memory():
    for i in range(torch.cuda.device_count()):
        allocated = torch.cuda.memory_allocated(i) / 1024**3
        reserved = torch.cuda.memory_reserved(i) / 1024**3
        print(f"GPU {i}: {allocated:.2f}GB / {reserved:.2f}GB")
```

### 2. å‰ç¼€ç¼“å­˜

```python
# å¯ç”¨å‰ç¼€ç¼“å­˜
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b",
    enable_prefix_caching=True,  # å¯ç”¨å‰ç¼€ç¼“å­˜
)

# å‰ç¼€ç¼“å­˜çš„å¥½å¤„ï¼š
# å¦‚æœå¤šä¸ªè¯·æ±‚æœ‰ç›¸åŒçš„å‰ç¼€ï¼ˆå¦‚ç³»ç»Ÿæç¤ºï¼‰ï¼Œ
# vLLM ä¼šç¼“å­˜å‰ç¼€çš„ KV Cacheï¼Œé¿å…é‡å¤è®¡ç®—

# ç¤ºä¾‹ï¼š
system_prompt = "You are a helpful coding assistant."

# è¯·æ±‚ 1
prompt1 = system_prompt + "\nWrite a Python function"
# è®¡ç®—å®Œæ•´çš„ KV Cache

# è¯·æ±‚ 2
prompt2 = system_prompt + "\nWrite a JavaScript function"
# å¤ç”¨ system_prompt çš„ KV Cacheï¼Œåªè®¡ç®—æ–°éƒ¨åˆ†
```

### 3. é‡åŒ–

```python
# ä½¿ç”¨ AWQ é‡åŒ–ï¼ˆæ¨èï¼‰
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b-awq",  # AWQ é‡åŒ–æ¨¡å‹
    quantization="awq",
    dtype="float16"
)

# æˆ–ä½¿ç”¨ GPTQ é‡åŒ–
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b-gptq",
    quantization="gptq",
    dtype="float16"
)

# é‡åŒ–çš„å¥½å¤„ï¼š
# - å†…å­˜ä½¿ç”¨å‡å°‘ 50-75%
# - ååé‡æå‡ 1.5-2x
# - ç²¾åº¦æŸå¤± < 1%
```

### 4. Tensor Parallelism é…ç½®

```python
# å• GPUï¼ˆå°æ¨¡å‹ï¼‰
engine_args = AsyncEngineArgs(
    model="/models/llama-3-8b",
    tensor_parallel_size=1
)

# 2 GPUï¼ˆä¸­ç­‰æ¨¡å‹ï¼‰
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b",
    tensor_parallel_size=2
)

# 4 GPUï¼ˆå¤§æ¨¡å‹ï¼‰
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b",
    tensor_parallel_size=4
)

# 8 GPUï¼ˆè¶…å¤§æ¨¡å‹ï¼‰
engine_args = AsyncEngineArgs(
    model="/models/llama-3-405b",
    tensor_parallel_size=8
)

# è§„åˆ™ï¼š
# - tensor_parallel_size å¿…é¡»èƒ½æ•´é™¤ GPU æ•°é‡
# - é€šä¿¡å¼€é”€éš TP å¢åŠ è€Œå¢åŠ 
# - å»ºè®®ï¼šæ¨¡å‹å¤§å° / GPU æ•°é‡ = æ¯ä¸ª GPU 20-40GB
```

---

## ğŸ³ ç”Ÿäº§éƒ¨ç½²

### Docker éƒ¨ç½²

```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# å®‰è£… Python
RUN apt-get update && apt-get install -y \
    python3.11 python3.11-venv python3-pip \
    git wget

# åˆ›å»ºå·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£… vLLM
RUN pip install vllm torch transformers

# å¤åˆ¶ä»£ç 
COPY . /app

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
# æ„å»ºé•œåƒ
docker build -t vlinders-server:latest .

# è¿è¡Œå®¹å™¨
docker run --gpus all \
  -p 8000:8000 \
  -v /models:/models \
  -e CUDA_VISIBLE_DEVICES=0,1,2,3 \
  vlinders-server:latest
```

### Kubernetes éƒ¨ç½²

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vlinders-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vlinders-server
  template:
    metadata:
      labels:
        app: vlinders-server
    spec:
      containers:
      - name: vlinders-server
        image: vlinders-server:latest
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 4  # æ¯ä¸ª Pod 4 ä¸ª GPU
            memory: 128Gi
            cpu: 16
          requests:
            nvidia.com/gpu: 4
            memory: 128Gi
            cpu: 16
        volumeMounts:
        - name: models
          mountPath: /models
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: models-pvc
```

### ç›‘æ§å’Œæ—¥å¿—

```python
# æ·»åŠ  Prometheus æŒ‡æ ‡
from prometheus_client import Counter, Histogram, Gauge

# è¯·æ±‚è®¡æ•°
request_count = Counter(
    'vllm_requests_total',
    'Total requests',
    ['model', 'status']
)

# å“åº”æ—¶é—´
response_time = Histogram(
    'vllm_response_seconds',
    'Response time',
    ['model']
)

# GPU ä½¿ç”¨ç‡
gpu_utilization = Gauge(
    'vllm_gpu_utilization',
    'GPU utilization',
    ['gpu_id']
)

# åœ¨æ¨ç†æ—¶è®°å½•æŒ‡æ ‡
async def generate_with_metrics(engine, prompt):
    start_time = time.time()

    try:
        result = await generate_text(engine, prompt)
        request_count.labels(model='llama-3-70b', status='success').inc()
        return result
    except Exception as e:
        request_count.labels(model='llama-3-70b', status='error').inc()
        raise
    finally:
        duration = time.time() - start_time
        response_time.labels(model='llama-3-70b').observe(duration)
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å‚æ•°è°ƒä¼˜

```python
# æ¨èé…ç½®ï¼ˆLlama 3 70B, 4x A100 80GBï¼‰
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b",
    tensor_parallel_size=4,
    dtype="float16",
    max_model_len=32768,
    gpu_memory_utilization=0.9,
    enable_prefix_caching=True,
    disable_log_stats=False,
    max_num_seqs=256,  # æœ€å¤§å¹¶å‘åºåˆ—æ•°
    max_num_batched_tokens=32768  # æœ€å¤§æ‰¹å¤„ç† tokens
)
```

### 2. å¸¸è§é—®é¢˜

**Q: OOM (Out of Memory) é”™è¯¯**
```python
# è§£å†³æ–¹æ¡ˆï¼š
# 1. é™ä½ gpu_memory_utilization
gpu_memory_utilization=0.85  # ä» 0.9 é™åˆ° 0.85

# 2. å‡å°‘ max_model_len
max_model_len=16384  # ä» 32768 é™åˆ° 16384

# 3. å¢åŠ  tensor_parallel_size
tensor_parallel_size=8  # ä» 4 å¢åŠ åˆ° 8
```

**Q: æ¨ç†é€Ÿåº¦æ…¢**
```python
# è§£å†³æ–¹æ¡ˆï¼š
# 1. å¯ç”¨å‰ç¼€ç¼“å­˜
enable_prefix_caching=True

# 2. ä½¿ç”¨é‡åŒ–
quantization="awq"

# 3. å¢åŠ æ‰¹å¤„ç†å¤§å°
max_num_seqs=512
```

### 3. æ€§èƒ½åŸºå‡†

```python
# æ€§èƒ½æµ‹è¯•è„šæœ¬
import asyncio
import time

async def benchmark(engine, num_requests=100):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    prompts = [f"Request {i}: Write code" for i in range(num_requests)]

    start_time = time.time()

    tasks = [
        generate_text(engine, prompt, max_tokens=100)
        for prompt in prompts
    ]

    results = await asyncio.gather(*tasks)

    end_time = time.time()
    duration = end_time - start_time

    print(f"Total requests: {num_requests}")
    print(f"Total time: {duration:.2f}s")
    print(f"Throughput: {num_requests / duration:.2f} req/s")
    print(f"Average latency: {duration / num_requests * 1000:.2f}ms")

# è¿è¡ŒåŸºå‡†æµ‹è¯•
await benchmark(engine, num_requests=100)
```

---

## ğŸ“ æ€»ç»“

### vLLM é›†æˆæ£€æŸ¥æ¸…å•

- [ ] å®‰è£… CUDA 12.1+ å’Œ cuDNN 8.9+
- [ ] å®‰è£… vLLM å’Œä¾èµ–
- [ ] ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
- [ ] é…ç½® Tensor Parallelism
- [ ] å¯ç”¨å‰ç¼€ç¼“å­˜
- [ ] å®ç°å¼‚æ­¥æ¨ç†æ¥å£
- [ ] å®ç°æµå¼ç”Ÿæˆ
- [ ] æ·»åŠ ç›‘æ§æŒ‡æ ‡
- [ ] Docker å®¹å™¨åŒ–
- [ ] Kubernetes éƒ¨ç½²
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### å…³é”®é…ç½®å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `tensor_parallel_size` | 4 | æ ¹æ®æ¨¡å‹å¤§å°å’Œ GPU æ•°é‡ |
| `gpu_memory_utilization` | 0.9 | ç•™ 10% ç»™ç³»ç»Ÿ |
| `max_model_len` | 32768 | æ ¹æ®éœ€æ±‚è°ƒæ•´ |
| `enable_prefix_caching` | True | æå‡æ€§èƒ½ |
| `dtype` | float16 | å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦ |

---

**ä¸‹ä¸€æ­¥**: é˜…è¯» [04-Agentç¼–æ’ç³»ç»Ÿ.md](./04-Agentç¼–æ’ç³»ç»Ÿ.md)
