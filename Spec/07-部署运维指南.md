# éƒ¨ç½²è¿ç»´æŒ‡å—

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-28
**æ–‡æ¡£ç±»å‹**: è¿ç»´æ‰‹å†Œ

---

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾› Vlinders-Server çš„å®Œæ•´éƒ¨ç½²è¿ç»´æŒ‡å—ï¼ŒåŒ…æ‹¬ Docker éƒ¨ç½²ã€Kubernetes éƒ¨ç½²ã€ç›‘æ§å‘Šè­¦ã€æ‰©ç¼©å®¹ã€å¤‡ä»½æ¢å¤å’Œæ•…éšœæ’æŸ¥ã€‚

---

## ğŸ—ï¸ éƒ¨ç½²æ¦‚è¿°

### ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è´Ÿè½½å‡è¡¡å±‚                             â”‚
â”‚                  (Nginx / Ingress)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server 1  â”‚  â”‚ API Server 2 â”‚  â”‚  API Server N  â”‚
â”‚  (FastAPI)     â”‚  â”‚  (FastAPI)   â”‚  â”‚  (FastAPI)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  vLLM Engine 1 â”‚  â”‚ vLLM Engine 2â”‚  â”‚ vLLM Engine N  â”‚
â”‚  (4x GPU)      â”‚  â”‚  (4x GPU)    â”‚  â”‚  (4x GPU)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis Cache   â”‚                    â”‚  PostgreSQL DB  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç¯å¢ƒè¦æ±‚

**ç¡¬ä»¶è¦æ±‚**:

| ç»„ä»¶ | æœ€å°é…ç½® | æ¨èé…ç½® | ç”Ÿäº§é…ç½® |
|------|---------|---------|---------|
| GPU | 1x A100 40GB | 4x A100 80GB | 8x H100 80GB |
| CPU | 16 æ ¸å¿ƒ | 32 æ ¸å¿ƒ | 64 æ ¸å¿ƒ |
| å†…å­˜ | 64GB | 128GB | 256GB |
| å­˜å‚¨ | 500GB SSD | 1TB NVMe | 2TB NVMe RAID |
| ç½‘ç»œ | 1Gbps | 10Gbps | 25Gbps |

**è½¯ä»¶è¦æ±‚**:

| è½¯ä»¶ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|
| OS | Ubuntu 22.04 LTS | æ¨èä½¿ç”¨ LTS ç‰ˆæœ¬ |
| Docker | 24.0+ | å®¹å™¨è¿è¡Œæ—¶ |
| Kubernetes | 1.28+ | å®¹å™¨ç¼–æ’ |
| NVIDIA Driver | 535+ | GPU é©±åŠ¨ |
| CUDA | 12.1+ | GPU è®¡ç®—å¹³å° |
| Python | 3.11 | è¿è¡Œæ—¶ç¯å¢ƒ |

### èµ„æºè§„åˆ’

**å•èŠ‚ç‚¹é…ç½®** (å¼€å‘/æµ‹è¯•):
- 1 ä¸ª API Server
- 1 ä¸ª vLLM Engine (1-2 GPU)
- 1 ä¸ª Redis
- 1 ä¸ª PostgreSQL

**å°è§„æ¨¡é›†ç¾¤** (å°å‹ç”Ÿäº§):
- 2-3 ä¸ª API Server
- 2-3 ä¸ª vLLM Engine (4 GPU æ¯ä¸ª)
- 1 ä¸ª Redis (ä¸»ä»)
- 1 ä¸ª PostgreSQL (ä¸»ä»)

**å¤§è§„æ¨¡é›†ç¾¤** (å¤§å‹ç”Ÿäº§):
- 5-10 ä¸ª API Server
- 5-10 ä¸ª vLLM Engine (4-8 GPU æ¯ä¸ª)
- Redis Cluster (3 ä¸» 3 ä»)
- PostgreSQL Cluster (1 ä¸» 2 ä»)

---

## ğŸ³ Docker éƒ¨ç½²

### Dockerfile

åˆ›å»º `Dockerfile`:

```dockerfile
# åŸºç¡€é•œåƒ
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=$CUDA_HOME/bin:$PATH
ENV LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-venv \
    python3-pip \
    git \
    wget \
    curl \
    vim \
    htop \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºå·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºé root ç”¨æˆ·
RUN useradd -m -u 1000 vlinders && \
    chown -R vlinders:vlinders /app

USER vlinders

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3.11", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### å¤šé˜¶æ®µæ„å»ºä¼˜åŒ–

åˆ›å»º `Dockerfile.optimized`:

```dockerfile
# æ„å»ºé˜¶æ®µ
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS builder

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    python3.11 python3-pip git && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /build

COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# è¿è¡Œé˜¶æ®µ
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

ENV PYTHONUNBUFFERED=1
ENV PATH=/root/.local/bin:$PATH

RUN apt-get update && apt-get install -y \
    python3.11 curl && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /app

# ä»æ„å»ºé˜¶æ®µå¤åˆ¶ä¾èµ–
COPY --from=builder /root/.local /root/.local

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["python3.11", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Docker Compose

åˆ›å»º `docker-compose.yml`:

```yaml
version: '3.8'

services:
  # API Server
  api-server:
    build:
      context: .
      dockerfile: Dockerfile
    image: vlinders-server:latest
    container_name: vlinders-api
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://vlinders:password@postgres:5432/vlinders
      - MODEL_PATH=/models/llama-3-70b
      - TENSOR_PARALLEL_SIZE=4
      - GPU_MEMORY_UTILIZATION=0.9
    volumes:
      - ./models:/models:ro
      - ./logs:/app/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 4
              capabilities: [gpu]
    depends_on:
      - redis
      - postgres
    restart: unless-stopped
    networks:
      - vlinders-network

  # Redis
  redis:
    image: redis:7-alpine
    container_name: vlinders-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 4gb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    networks:
      - vlinders-network

  # PostgreSQL
  postgres:
    image: postgres:15-alpine
    container_name: vlinders-postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=vlinders
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=vlinders
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped
    networks:
      - vlinders-network

  # Nginx (è´Ÿè½½å‡è¡¡)
  nginx:
    image: nginx:alpine
    container_name: vlinders-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - api-server
    restart: unless-stopped
    networks:
      - vlinders-network

  # Prometheus (ç›‘æ§)
  prometheus:
    image: prom/prometheus:latest
    container_name: vlinders-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    restart: unless-stopped
    networks:
      - vlinders-network

  # Grafana (å¯è§†åŒ–)
  grafana:
    image: grafana/grafana:latest
    container_name: vlinders-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - vlinders-network

volumes:
  redis-data:
  postgres-data:
  prometheus-data:
  grafana-data:

networks:
  vlinders-network:
    driver: bridge
```

### éƒ¨ç½²å‘½ä»¤

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f api-server

# æŸ¥çœ‹çŠ¶æ€
docker-compose ps

# åœæ­¢æœåŠ¡
docker-compose down

# é‡å¯æœåŠ¡
docker-compose restart api-server

# æ‰©å±• API Server
docker-compose up -d --scale api-server=3
```

### é•œåƒä¼˜åŒ–

**å‡å°é•œåƒå¤§å°**:

```dockerfile
# ä½¿ç”¨ slim åŸºç¡€é•œåƒ
FROM python:3.11-slim

# æ¸…ç†ç¼“å­˜
RUN apt-get clean && rm -rf /var/lib/apt/lists/*

# ä½¿ç”¨ --no-cache-dir
RUN pip install --no-cache-dir -r requirements.txt

# åˆ é™¤ä¸å¿…è¦çš„æ–‡ä»¶
RUN find /usr/local -type d -name '__pycache__' -exec rm -rf {} +
```

**å¤šæ¶æ„æ”¯æŒ**:

```bash
# æ„å»ºå¤šæ¶æ„é•œåƒ
docker buildx build --platform linux/amd64,linux/arm64 \
  -t vlinders-server:latest \
  --push .
```

---

## â˜¸ï¸ Kubernetes éƒ¨ç½²

### å‘½åç©ºé—´

åˆ›å»º `namespace.yaml`:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: vlinders
  labels:
    name: vlinders
```

### ConfigMap

åˆ›å»º `configmap.yaml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vlinders-config
  namespace: vlinders
data:
  # åº”ç”¨é…ç½®
  MODEL_PATH: "/models/llama-3-70b"
  TENSOR_PARALLEL_SIZE: "4"
  GPU_MEMORY_UTILIZATION: "0.9"
  MAX_MODEL_LEN: "32768"
  ENABLE_PREFIX_CACHING: "true"

  # Redis é…ç½®
  REDIS_URL: "redis://vlinders-redis:6379"
  REDIS_MAX_CONNECTIONS: "100"

  # æ•°æ®åº“é…ç½®
  DATABASE_URL: "postgresql://vlinders:password@vlinders-postgres:5432/vlinders"
  DATABASE_POOL_SIZE: "20"

  # æ—¥å¿—é…ç½®
  LOG_LEVEL: "INFO"
  LOG_FORMAT: "json"
```

### Secret

åˆ›å»º `secret.yaml`:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: vlinders-secret
  namespace: vlinders
type: Opaque
stringData:
  # æ•°æ®åº“å¯†ç 
  POSTGRES_PASSWORD: "your-secure-password"

  # Redis å¯†ç 
  REDIS_PASSWORD: "your-redis-password"

  # API å¯†é’¥
  API_SECRET_KEY: "your-api-secret-key"

  # JWT å¯†é’¥
  JWT_SECRET_KEY: "your-jwt-secret-key"
```

### PersistentVolumeClaim

åˆ›å»º `pvc.yaml`:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: models-pvc
  namespace: vlinders
spec:
  accessModes:
    - ReadOnlyMany
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 500Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: logs-pvc
  namespace: vlinders
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: standard
  resources:
    requests:
      storage: 100Gi
```

### Deployment

åˆ›å»º `deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vlinders-server
  namespace: vlinders
  labels:
    app: vlinders-server
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: vlinders-server
  template:
    metadata:
      labels:
        app: vlinders-server
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      # GPU èŠ‚ç‚¹é€‰æ‹©
      nodeSelector:
        nvidia.com/gpu: "true"
        node-type: gpu-worker

      # å®¹å¿ GPU æ±¡ç‚¹
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

      # åˆå§‹åŒ–å®¹å™¨
      initContainers:
      - name: wait-for-redis
        image: busybox:1.35
        command: ['sh', '-c', 'until nc -z vlinders-redis 6379; do echo waiting for redis; sleep 2; done;']

      - name: wait-for-postgres
        image: busybox:1.35
        command: ['sh', '-c', 'until nc -z vlinders-postgres 5432; do echo waiting for postgres; sleep 2; done;']

      containers:
      - name: vlinders-server
        image: vlinders-server:v1.0.0
        imagePullPolicy: IfNotPresent

        ports:
        - name: http
          containerPort: 8000
          protocol: TCP

        # ç¯å¢ƒå˜é‡
        envFrom:
        - configMapRef:
            name: vlinders-config
        - secretRef:
            name: vlinders-secret

        # èµ„æºé™åˆ¶
        resources:
          limits:
            nvidia.com/gpu: 4
            memory: 128Gi
            cpu: 16
          requests:
            nvidia.com/gpu: 4
            memory: 128Gi
            cpu: 16

        # å­˜å‚¨å·
        volumeMounts:
        - name: models
          mountPath: /models
          readOnly: true
        - name: logs
          mountPath: /app/logs

        # å¥åº·æ£€æŸ¥
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        # å¯åŠ¨æ¢é’ˆ
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30

      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: models-pvc
      - name: logs
        persistentVolumeClaim:
          claimName: logs-pvc

      # ä¼˜é›…å…³é—­
      terminationGracePeriodSeconds: 60
```

### Service

åˆ›å»º `service.yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: vlinders-server
  namespace: vlinders
  labels:
    app: vlinders-server
spec:
  type: ClusterIP
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: vlinders-server
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600
---
apiVersion: v1
kind: Service
metadata:
  name: vlinders-server-headless
  namespace: vlinders
  labels:
    app: vlinders-server
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 8000
    targetPort: 8000
    protocol: TCP
    name: http
  selector:
    app: vlinders-server
```

### Ingress

åˆ›å»º `ingress.yaml`:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vlinders-ingress
  namespace: vlinders
  annotations:
    # Nginx é…ç½®
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"

    # è¶…æ—¶é…ç½®
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"

    # è¯·æ±‚å¤§å°é™åˆ¶
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"

    # é€Ÿç‡é™åˆ¶
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/limit-connections: "50"

    # CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE, OPTIONS"

    # SSL è¯ä¹¦
    cert-manager.io/cluster-issuer: "letsencrypt-prod"

spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - api.vlinders.ai
    secretName: vlinders-tls
  rules:
  - host: api.vlinders.ai
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: vlinders-server
            port:
              number: 8000
```

### HorizontalPodAutoscaler

åˆ›å»º `hpa.yaml`:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vlinders-server-hpa
  namespace: vlinders
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vlinders-server
  minReplicas: 2
  maxReplicas: 10
  metrics:
  # CPU ä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # å†…å­˜ä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # GPU ä½¿ç”¨ç‡ (éœ€è¦ DCGM Exporter)
  - type: Pods
    pods:
      metric:
        name: gpu_utilization
      target:
        type: AverageValue
        averageValue: "75"

  # è‡ªå®šä¹‰æŒ‡æ ‡ï¼šè¯·æ±‚é˜Ÿåˆ—é•¿åº¦
  - type: Pods
    pods:
      metric:
        name: request_queue_length
      target:
        type: AverageValue
        averageValue: "10"

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 1
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 30
      selectPolicy: Max
```

### éƒ¨ç½²å‘½ä»¤

```bash
# åˆ›å»ºå‘½åç©ºé—´
kubectl apply -f namespace.yaml

# åˆ›å»ºé…ç½®
kubectl apply -f configmap.yaml
kubectl apply -f secret.yaml

# åˆ›å»ºå­˜å‚¨
kubectl apply -f pvc.yaml

# éƒ¨ç½²åº”ç”¨
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
kubectl apply -f ingress.yaml

# é…ç½®è‡ªåŠ¨æ‰©ç¼©å®¹
kubectl apply -f hpa.yaml

# æŸ¥çœ‹çŠ¶æ€
kubectl get all -n vlinders

# æŸ¥çœ‹ Pod æ—¥å¿—
kubectl logs -f -n vlinders deployment/vlinders-server

# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
kubectl describe node <node-name> | grep -A 10 "Allocated resources"

# è¿›å…¥å®¹å™¨
kubectl exec -it -n vlinders <pod-name> -- /bin/bash

# æŸ¥çœ‹ HPA çŠ¶æ€
kubectl get hpa -n vlinders

# æŸ¥çœ‹äº‹ä»¶
kubectl get events -n vlinders --sort-by='.lastTimestamp'
```

---

## âš–ï¸ è´Ÿè½½å‡è¡¡

### Nginx é…ç½®

åˆ›å»º `nginx.conf`:

```nginx
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # æ—¥å¿—æ ¼å¼
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    access_log /var/log/nginx/access.log main;

    # æ€§èƒ½ä¼˜åŒ–
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 10M;

    # Gzip å‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css text/xml text/javascript
               application/json application/javascript application/xml+rss
               application/rss+xml font/truetype font/opentype
               application/vnd.ms-fontobject image/svg+xml;

    # ä¸Šæ¸¸æœåŠ¡å™¨
    upstream vlinders_backend {
        least_conn;  # æœ€å°‘è¿æ¥è´Ÿè½½å‡è¡¡

        server api-server-1:8000 max_fails=3 fail_timeout=30s;
        server api-server-2:8000 max_fails=3 fail_timeout=30s;
        server api-server-3:8000 max_fails=3 fail_timeout=30s;

        keepalive 32;
    }

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=100r/s;
    limit_conn_zone $binary_remote_addr zone=conn_limit:10m;

    # HTTP æœåŠ¡å™¨
    server {
        listen 80;
        server_name api.vlinders.ai;

        # é‡å®šå‘åˆ° HTTPS
        return 301 https://$server_name$request_uri;
    }

    # HTTPS æœåŠ¡å™¨
    server {
        listen 443 ssl http2;
        server_name api.vlinders.ai;

        # SSL è¯ä¹¦
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;

        # SSL é…ç½®
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        ssl_prefer_server_ciphers on;
        ssl_session_cache shared:SSL:10m;
        ssl_session_timeout 10m;

        # å®‰å…¨å¤´
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;

        # API è·¯ç”±
        location / {
            # é™æµ
            limit_req zone=api_limit burst=20 nodelay;
            limit_conn conn_limit 10;

            # ä»£ç†é…ç½®
            proxy_pass http://vlinders_backend;
            proxy_http_version 1.1;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header Connection "";

            # è¶…æ—¶é…ç½®
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;

            # ç¼“å†²é…ç½®
            proxy_buffering off;
            proxy_request_buffering off;
        }

        # æµå¼å“åº”
        location /v1/chat/completions {
            proxy_pass http://vlinders_backend;
            proxy_http_version 1.1;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Connection "";

            # ç¦ç”¨ç¼“å†²ä»¥æ”¯æŒæµå¼å“åº”
            proxy_buffering off;
            proxy_cache off;
            proxy_request_buffering off;

            # è¶…æ—¶é…ç½®
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;

            # æ”¯æŒ SSE
            chunked_transfer_encoding on;
        }

        # å¥åº·æ£€æŸ¥
        location /health {
            access_log off;
            proxy_pass http://vlinders_backend/health;
        }

        # ç›‘æ§æŒ‡æ ‡
        location /metrics {
            access_log off;
            proxy_pass http://vlinders_backend/metrics;

            # é™åˆ¶è®¿é—®
            allow 10.0.0.0/8;
            deny all;
        }

        # Nginx çŠ¶æ€
        location /nginx_status {
            stub_status on;
            access_log off;

            allow 10.0.0.0/8;
            deny all;
        }
    }
}
```

### å¥åº·æ£€æŸ¥

åœ¨åº”ç”¨ä¸­å®ç°å¥åº·æ£€æŸ¥ç«¯ç‚¹:

```python
from fastapi import FastAPI, status
from fastapi.responses import JSONResponse

app = FastAPI()

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"status": "healthy"}
    )

@app.get("/ready")
async def readiness_check():
    """å°±ç»ªæ£€æŸ¥"""
    try:
        # æ£€æŸ¥ vLLM å¼•æ“
        if not engine_manager.is_ready():
            return JSONResponse(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                content={"status": "not ready", "reason": "engine not loaded"}
            )

        # æ£€æŸ¥ Redis
        await redis_client.ping()

        # æ£€æŸ¥æ•°æ®åº“
        await db.execute("SELECT 1")

        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={"status": "ready"}
        )
    except Exception as e:
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={"status": "not ready", "reason": str(e)}
        )
```

---

## ğŸ“Š ç›‘æ§å‘Šè­¦

### Prometheus é…ç½®

åˆ›å»º `prometheus.yml`:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'vlinders-prod'
    environment: 'production'

# å‘Šè­¦è§„åˆ™
rule_files:
  - '/etc/prometheus/rules/*.yml'

# Alertmanager é…ç½®
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - alertmanager:9093

# æŠ“å–é…ç½®
scrape_configs:
  # Prometheus è‡ªèº«
  - job_name: 'prometheus'
    static_configs:
    - targets: ['localhost:9090']

  # Vlinders Server
  - job_name: 'vlinders-server'
    kubernetes_sd_configs:
    - role: pod
      namespaces:
        names:
        - vlinders
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
      action: replace
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2
      target_label: __address__

  # Node Exporter
  - job_name: 'node-exporter'
    kubernetes_sd_configs:
    - role: node
    relabel_configs:
    - source_labels: [__address__]
      regex: '(.*):10250'
      replacement: '${1}:9100'
      target_label: __address__

  # DCGM Exporter (GPU ç›‘æ§)
  - job_name: 'dcgm-exporter'
    kubernetes_sd_configs:
    - role: pod
      namespaces:
        names:
        - gpu-operator
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_label_app]
      action: keep
      regex: nvidia-dcgm-exporter

  # Redis Exporter
  - job_name: 'redis-exporter'
    static_configs:
    - targets: ['redis-exporter:9121']

  # PostgreSQL Exporter
  - job_name: 'postgres-exporter'
    static_configs:
    - targets: ['postgres-exporter:9187']
```

### å‘Šè­¦è§„åˆ™

åˆ›å»º `alerts.yml`:

```yaml
groups:
  # åº”ç”¨å‘Šè­¦
  - name: vlinders_alerts
    interval: 30s
    rules:
    # æœåŠ¡ä¸å¯ç”¨
    - alert: ServiceDown
      expr: up{job="vlinders-server"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Vlinders Server is down"
        description: "{{ $labels.instance }} has been down for more than 1 minute"

    # é«˜é”™è¯¯ç‡
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

    # é«˜å»¶è¿Ÿ
    - alert: HighLatency
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High latency detected"
        description: "P95 latency is {{ $value }}s on {{ $labels.instance }}"

    # è¯·æ±‚é˜Ÿåˆ—è¿‡é•¿
    - alert: LongRequestQueue
      expr: vllm_request_queue_length > 50
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Request queue is too long"
        description: "Queue length is {{ $value }} on {{ $labels.instance }}"

  # GPU å‘Šè­¦
  - name: gpu_alerts
    interval: 30s
    rules:
    # GPU ä½¿ç”¨ç‡è¿‡é«˜
    - alert: HighGPUUtilization
      expr: DCGM_FI_DEV_GPU_UTIL > 95
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High GPU utilization"
        description: "GPU {{ $labels.gpu }} utilization is {{ $value }}% on {{ $labels.instance }}"

    # GPU å†…å­˜ä¸è¶³
    - alert: GPUMemoryLow
      expr: (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_FREE) > 0.95
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "GPU memory is low"
        description: "GPU {{ $labels.gpu }} memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

    # GPU æ¸©åº¦è¿‡é«˜
    - alert: HighGPUTemperature
      expr: DCGM_FI_DEV_GPU_TEMP > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High GPU temperature"
        description: "GPU {{ $labels.gpu }} temperature is {{ $value }}Â°C on {{ $labels.instance }}"

  # èµ„æºå‘Šè­¦
  - name: resource_alerts
    interval: 30s
    rules:
    # CPU ä½¿ç”¨ç‡è¿‡é«˜
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage"
        description: "CPU usage is {{ $value | humanize }}% on {{ $labels.instance }}"

    # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage"
        description: "Memory usage is {{ $value | humanize }}% on {{ $labels.instance }}"

    # ç£ç›˜ç©ºé—´ä¸è¶³
    - alert: DiskSpaceLow
      expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Disk space is low"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }} ({{ $labels.mountpoint }})"

  # æ•°æ®åº“å‘Šè­¦
  - name: database_alerts
    interval: 30s
    rules:
    # æ•°æ®åº“è¿æ¥æ•°è¿‡å¤š
    - alert: HighDatabaseConnections
      expr: pg_stat_database_numbackends > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High database connections"
        description: "Database has {{ $value }} connections"

    # æ…¢æŸ¥è¯¢
    - alert: SlowQueries
      expr: rate(pg_stat_statements_mean_exec_time[5m]) > 1000
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Slow queries detected"
        description: "Average query time is {{ $value }}ms"
```

### Grafana ä»ªè¡¨æ¿

åˆ›å»º `grafana-dashboard.json`:

```json
{
  "dashboard": {
    "title": "Vlinders Server Monitoring",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])"
          }
        ]
      },
      {
        "title": "Response Time (P95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
          }
        ]
      },
      {
        "title": "GPU Utilization",
        "targets": [
          {
            "expr": "DCGM_FI_DEV_GPU_UTIL"
          }
        ]
      },
      {
        "title": "GPU Memory Usage",
        "targets": [
          {
            "expr": "DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_FREE * 100"
          }
        ]
      },
      {
        "title": "Request Queue Length",
        "targets": [
          {
            "expr": "vllm_request_queue_length"
          }
        ]
      },
      {
        "title": "Token Throughput",
        "targets": [
          {
            "expr": "rate(vllm_tokens_generated_total[5m])"
          }
        ]
      }
    ]
  }
}
```

### æ—¥å¿—æ”¶é›†

ä½¿ç”¨ Fluentd æ”¶é›†æ—¥å¿—ï¼Œåˆ›å»º `fluentd-config.yaml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: vlinders
data:
  fluent.conf: |
    # è¾“å…¥ï¼šä»å®¹å™¨æ—¥å¿—æ”¶é›†
    <source>
      @type tail
      path /var/log/containers/vlinders-server-*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    # è¿‡æ»¤ï¼šæ·»åŠ  Kubernetes å…ƒæ•°æ®
    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
    </filter>

    # è¿‡æ»¤ï¼šè§£æåº”ç”¨æ—¥å¿—
    <filter kubernetes.var.log.containers.vlinders-server-**>
      @type parser
      key_name log
      <parse>
        @type json
      </parse>
    </filter>

    # è¾“å‡ºï¼šå‘é€åˆ° Elasticsearch
    <match kubernetes.**>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      logstash_format true
      logstash_prefix vlinders
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_interval 5s
        retry_forever false
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      </buffer>
    </match>
```

---

## ğŸ”„ æ‰©ç¼©å®¹

### æ‰‹åŠ¨æ‰©ç¼©å®¹

```bash
# æ‰©å±•åˆ° 5 ä¸ªå‰¯æœ¬
kubectl scale deployment vlinders-server -n vlinders --replicas=5

# ç¼©å‡åˆ° 2 ä¸ªå‰¯æœ¬
kubectl scale deployment vlinders-server -n vlinders --replicas=2

# æŸ¥çœ‹æ‰©ç¼©å®¹çŠ¶æ€
kubectl get deployment vlinders-server -n vlinders -w
```

### è‡ªåŠ¨æ‰©ç¼©å®¹ (HPA)

HPA é…ç½®å·²åœ¨å‰é¢çš„ `hpa.yaml` ä¸­å®šä¹‰ï¼ŒåŸºäºä»¥ä¸‹æŒ‡æ ‡è‡ªåŠ¨æ‰©ç¼©å®¹ï¼š
- CPU ä½¿ç”¨ç‡ > 70%
- å†…å­˜ä½¿ç”¨ç‡ > 80%
- GPU ä½¿ç”¨ç‡ > 75%
- è¯·æ±‚é˜Ÿåˆ—é•¿åº¦ > 10

### å‚ç›´æ‰©ç¼©å®¹ (VPA)

åˆ›å»º `vpa.yaml`:

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: vlinders-server-vpa
  namespace: vlinders
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vlinders-server
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: vlinders-server
      minAllowed:
        cpu: 8
        memory: 64Gi
      maxAllowed:
        cpu: 32
        memory: 256Gi
      controlledResources:
      - cpu
      - memory
```

### GPU è°ƒåº¦ç­–ç•¥

åˆ›å»º `gpu-scheduler-config.yaml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-scheduler-config
  namespace: kube-system
data:
  config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
    - schedulerName: gpu-scheduler
      plugins:
        score:
          enabled:
          - name: NodeResourcesFit
            weight: 1
          - name: GPUDevicePlugin
            weight: 10
      pluginConfig:
      - name: NodeResourcesFit
        args:
          scoringStrategy:
            type: MostAllocated
            resources:
            - name: nvidia.com/gpu
              weight: 10
```

---

## ğŸ’¾ å¤‡ä»½æ¢å¤

### æ•°æ®åº“å¤‡ä»½

åˆ›å»º `backup-cronjob.yaml`:

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: vlinders
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨ 2 ç‚¹
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:15-alpine
            env:
            - name: PGHOST
              value: vlinders-postgres
            - name: PGPORT
              value: "5432"
            - name: PGUSER
              value: vlinders
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: vlinders-secret
                  key: POSTGRES_PASSWORD
            - name: PGDATABASE
              value: vlinders
            - name: BACKUP_DIR
              value: /backups
            command:
            - /bin/sh
            - -c
            - |
              BACKUP_FILE="${BACKUP_DIR}/backup-$(date +%Y%m%d-%H%M%S).sql.gz"
              pg_dump | gzip > "${BACKUP_FILE}"
              echo "Backup created: ${BACKUP_FILE}"

              # åˆ é™¤ 30 å¤©å‰çš„å¤‡ä»½
              find ${BACKUP_DIR} -name "backup-*.sql.gz" -mtime +30 -delete
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
```

### æ¨¡å‹å¤‡ä»½

```bash
# å¤‡ä»½æ¨¡å‹åˆ° S3
aws s3 sync /models/llama-3-70b s3://vlinders-models/llama-3-70b \
  --storage-class GLACIER \
  --exclude "*.tmp"

# ä» S3 æ¢å¤æ¨¡å‹
aws s3 sync s3://vlinders-models/llama-3-70b /models/llama-3-70b
```

### é…ç½®å¤‡ä»½

```bash
# å¤‡ä»½ Kubernetes é…ç½®
kubectl get all,configmap,secret,pvc,ingress -n vlinders -o yaml > vlinders-backup.yaml

# æ¢å¤é…ç½®
kubectl apply -f vlinders-backup.yaml
```

### ç¾éš¾æ¢å¤è®¡åˆ’

**RTO (Recovery Time Objective)**: 1 å°æ—¶
**RPO (Recovery Point Objective)**: 24 å°æ—¶

**æ¢å¤æ­¥éª¤**:

1. **å‡†å¤‡æ–°ç¯å¢ƒ**
```bash
# åˆ›å»ºå‘½åç©ºé—´
kubectl apply -f namespace.yaml

# æ¢å¤é…ç½®
kubectl apply -f vlinders-backup.yaml
```

2. **æ¢å¤æ•°æ®åº“**
```bash
# æ¢å¤æœ€æ–°å¤‡ä»½
gunzip -c backup-20260228-020000.sql.gz | psql -h postgres -U vlinders -d vlinders
```

3. **æ¢å¤æ¨¡å‹**
```bash
# ä» S3 ä¸‹è½½æ¨¡å‹
aws s3 sync s3://vlinders-models/llama-3-70b /models/llama-3-70b
```

4. **å¯åŠ¨æœåŠ¡**
```bash
# éƒ¨ç½²åº”ç”¨
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
kubectl apply -f ingress.yaml

# éªŒè¯æœåŠ¡
kubectl get pods -n vlinders
curl https://api.vlinders.ai/health
```

---

## âš¡ æ€§èƒ½è°ƒä¼˜

### GPU ä¼˜åŒ–

**1. CUDA ä¼˜åŒ–**

```bash
# è®¾ç½® CUDA ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3
export CUDA_DEVICE_ORDER=PCI_BUS_ID

# å¯ç”¨ TF32
export NVIDIA_TF32_OVERRIDE=1

# ç¦ç”¨ CUDA ç¼“å­˜
export CUDA_CACHE_DISABLE=0
```

**2. vLLM ä¼˜åŒ–**

```python
# ä¼˜åŒ–é…ç½®
engine_args = AsyncEngineArgs(
    model="/models/llama-3-70b",
    tensor_parallel_size=4,
    dtype="float16",
    max_model_len=32768,
    gpu_memory_utilization=0.9,
    enable_prefix_caching=True,
    max_num_seqs=256,
    max_num_batched_tokens=32768,
    disable_log_stats=False,
    trust_remote_code=True
)
```

**3. GPU äº²å’Œæ€§**

```yaml
# åœ¨ Deployment ä¸­è®¾ç½® GPU äº²å’Œæ€§
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A100-SXM4-80GB
```

### ç½‘ç»œä¼˜åŒ–

**1. TCP å‚æ•°è°ƒä¼˜**

```bash
# ç¼–è¾‘ /etc/sysctl.conf
cat >> /etc/sysctl.conf <<EOF
# å¢åŠ  TCP ç¼“å†²åŒº
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 67108864
net.ipv4.tcp_wmem = 4096 65536 67108864

# å¯ç”¨ TCP Fast Open
net.ipv4.tcp_fastopen = 3

# å¢åŠ è¿æ¥é˜Ÿåˆ—
net.core.somaxconn = 4096
net.ipv4.tcp_max_syn_backlog = 8192

# å¯ç”¨ TCP BBR
net.core.default_qdisc = fq
net.ipv4.tcp_congestion_control = bbr
EOF

# åº”ç”¨é…ç½®
sysctl -p
```

**2. Nginx ä¼˜åŒ–**

```nginx
# å¢åŠ  worker è¿›ç¨‹
worker_processes auto;
worker_rlimit_nofile 65535;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    # å¯ç”¨ HTTP/2
    listen 443 ssl http2;

    # å¯ç”¨ keepalive
    keepalive_timeout 65;
    keepalive_requests 100;

    # ä¸Šæ¸¸ keepalive
    upstream vlinders_backend {
        keepalive 32;
        keepalive_requests 100;
        keepalive_timeout 60s;
    }
}
```

### å­˜å‚¨ä¼˜åŒ–

**1. ä½¿ç”¨ NVMe SSD**

```yaml
# StorageClass for NVMe
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-nvme
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: nvme
  fsType: ext4
  mountOptions:
  - noatime
  - nodiratime
```

**2. æ¨¡å‹ç¼“å­˜ä¼˜åŒ–**

```python
# ä½¿ç”¨ mmap åŠ è½½æ¨¡å‹
import mmap

def load_model_with_mmap(model_path):
    """ä½¿ç”¨ mmap åŠ è½½æ¨¡å‹"""
    with open(model_path, 'rb') as f:
        mmapped_file = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
        # åŠ è½½æ¨¡å‹
        return load_model(mmapped_file)
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

**1. Pod æ— æ³•å¯åŠ¨**

```bash
# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl describe pod <pod-name> -n vlinders

# æŸ¥çœ‹æ—¥å¿—
kubectl logs <pod-name> -n vlinders

# å¸¸è§åŸå› ï¼š
# - GPU èµ„æºä¸è¶³
# - é•œåƒæ‹‰å–å¤±è´¥
# - é…ç½®é”™è¯¯
# - å­˜å‚¨å·æŒ‚è½½å¤±è´¥
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥ GPU èµ„æº
kubectl describe node <node-name> | grep -A 10 "Allocated resources"

# æ£€æŸ¥é•œåƒ
kubectl get events -n vlinders | grep "Failed to pull image"

# æ£€æŸ¥é…ç½®
kubectl get configmap vlinders-config -n vlinders -o yaml

# æ£€æŸ¥å­˜å‚¨
kubectl get pvc -n vlinders
```

**2. OOM (Out of Memory)**

```bash
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
kubectl top pod -n vlinders

# æŸ¥çœ‹ OOM äº‹ä»¶
kubectl get events -n vlinders | grep OOMKilled
```

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# å¢åŠ å†…å­˜é™åˆ¶
resources:
  limits:
    memory: 256Gi
  requests:
    memory: 256Gi

# æˆ–é™ä½ GPU å†…å­˜ä½¿ç”¨ç‡
env:
- name: GPU_MEMORY_UTILIZATION
  value: "0.85"
```

**3. æ¨ç†é€Ÿåº¦æ…¢**

```bash
# æ£€æŸ¥ GPU ä½¿ç”¨ç‡
nvidia-smi

# æŸ¥çœ‹è¯·æ±‚é˜Ÿåˆ—
curl http://localhost:8000/metrics | grep queue_length

# æŸ¥çœ‹æ—¥å¿—
kubectl logs -f <pod-name> -n vlinders | grep "request_time"
```

**è§£å†³æ–¹æ¡ˆ**:

```python
# å¯ç”¨å‰ç¼€ç¼“å­˜
enable_prefix_caching=True

# å¢åŠ æ‰¹å¤„ç†å¤§å°
max_num_seqs=512

# ä½¿ç”¨é‡åŒ–æ¨¡å‹
quantization="awq"
```

**4. ç½‘ç»œè¶…æ—¶**

```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
kubectl exec -it <pod-name> -n vlinders -- curl -v http://vlinders-redis:6379

# æ£€æŸ¥ DNS
kubectl exec -it <pod-name> -n vlinders -- nslookup vlinders-redis

# æ£€æŸ¥é˜²ç«å¢™
iptables -L -n
```

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# å¢åŠ è¶…æ—¶æ—¶é—´
livenessProbe:
  timeoutSeconds: 30
readinessProbe:
  timeoutSeconds: 30

# æˆ–åœ¨ Nginx ä¸­å¢åŠ è¶…æ—¶
proxy_connect_timeout 300s;
proxy_send_timeout 300s;
proxy_read_timeout 300s;
```

### è°ƒè¯•æŠ€å·§

**1. è¿›å…¥å®¹å™¨è°ƒè¯•**

```bash
# è¿›å…¥å®¹å™¨
kubectl exec -it <pod-name> -n vlinders -- /bin/bash

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep python

# æŸ¥çœ‹ç½‘ç»œè¿æ¥
netstat -tunlp

# æŸ¥çœ‹ GPU
nvidia-smi

# æŸ¥çœ‹æ—¥å¿—
tail -f /app/logs/app.log
```

**2. ä½¿ç”¨ Debug å®¹å™¨**

```bash
# åˆ›å»º Debug å®¹å™¨
kubectl debug <pod-name> -n vlinders -it --image=ubuntu --share-processes

# åœ¨ Debug å®¹å™¨ä¸­å®‰è£…å·¥å…·
apt-get update && apt-get install -y curl vim htop
```

**3. ç½‘ç»œæŠ“åŒ…**

```bash
# åœ¨ Pod ä¸­æŠ“åŒ…
kubectl exec <pod-name> -n vlinders -- tcpdump -i any -w /tmp/capture.pcap

# ä¸‹è½½æŠ“åŒ…æ–‡ä»¶
kubectl cp vlinders/<pod-name>:/tmp/capture.pcap ./capture.pcap

# åˆ†ææŠ“åŒ…
wireshark capture.pcap
```

### æ—¥å¿—åˆ†æ

**1. æŸ¥çœ‹åº”ç”¨æ—¥å¿—**

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
kubectl logs -f <pod-name> -n vlinders

# æŸ¥çœ‹æœ€è¿‘ 100 è¡Œ
kubectl logs --tail=100 <pod-name> -n vlinders

# æŸ¥çœ‹ç‰¹å®šæ—¶é—´èŒƒå›´
kubectl logs --since=1h <pod-name> -n vlinders

# æŸ¥çœ‹æ‰€æœ‰å®¹å™¨æ—¥å¿—
kubectl logs <pod-name> -n vlinders --all-containers=true
```

**2. ä½¿ç”¨ Elasticsearch æŸ¥è¯¢æ—¥å¿—**

```bash
# æŸ¥è¯¢é”™è¯¯æ—¥å¿—
curl -X GET "elasticsearch:9200/vlinders-*/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "ERROR" } },
        { "range": { "@timestamp": { "gte": "now-1h" } } }
      ]
    }
  }
}'
```

**3. æ—¥å¿—èšåˆåˆ†æ**

```python
# ä½¿ç”¨ Python åˆ†ææ—¥å¿—
import json
from collections import Counter

def analyze_logs(log_file):
    """åˆ†ææ—¥å¿—æ–‡ä»¶"""
    errors = []
    response_times = []

    with open(log_file) as f:
        for line in f:
            log = json.loads(line)

            if log.get('level') == 'ERROR':
                errors.append(log.get('message'))

            if 'response_time' in log:
                response_times.append(log['response_time'])

    # ç»Ÿè®¡é”™è¯¯ç±»å‹
    error_counts = Counter(errors)
    print("Top 10 errors:")
    for error, count in error_counts.most_common(10):
        print(f"  {error}: {count}")

    # è®¡ç®—å“åº”æ—¶é—´ç»Ÿè®¡
    if response_times:
        avg_time = sum(response_times) / len(response_times)
        print(f"\nAverage response time: {avg_time:.2f}ms")
        print(f"Max response time: {max(response_times):.2f}ms")
```

---

## ğŸ”’ å®‰å…¨åŠ å›º

### ç½‘ç»œå®‰å…¨

**1. NetworkPolicy**

åˆ›å»º `network-policy.yaml`:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: vlinders-network-policy
  namespace: vlinders
spec:
  podSelector:
    matchLabels:
      app: vlinders-server
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # å…è®¸æ¥è‡ª Nginx çš„æµé‡
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8000
  # å…è®¸æ¥è‡ª Prometheus çš„æµé‡
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 8000
  egress:
  # å…è®¸è®¿é—® Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  # å…è®¸è®¿é—® PostgreSQL
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
  # å…è®¸ DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
```

**2. TLS åŠ å¯†**

```yaml
# ä½¿ç”¨ cert-manager è‡ªåŠ¨ç®¡ç†è¯ä¹¦
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: vlinders-tls
  namespace: vlinders
spec:
  secretName: vlinders-tls
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  dnsNames:
  - api.vlinders.ai
  - "*.vlinders.ai"
```

### è®¿é—®æ§åˆ¶

**1. RBAC**

åˆ›å»º `rbac.yaml`:

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vlinders-sa
  namespace: vlinders
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: vlinders-role
  namespace: vlinders
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: vlinders-rolebinding
  namespace: vlinders
subjects:
- kind: ServiceAccount
  name: vlinders-sa
  namespace: vlinders
roleRef:
  kind: Role
  name: vlinders-role
  apiGroup: rbac.authorization.k8s.io
```

**2. Pod Security Policy**

```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: vlinders-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - 'configMap'
  - 'emptyDir'
  - 'projected'
  - 'secret'
  - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: false
```

### æ•°æ®åŠ å¯†

**1. é™æ€æ•°æ®åŠ å¯†**

```yaml
# å¯ç”¨ etcd åŠ å¯†
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
- resources:
  - secrets
  - configmaps
  providers:
  - aescbc:
      keys:
      - name: key1
        secret: <base64-encoded-secret>
  - identity: {}
```

**2. ä¼ è¾“åŠ å¯†**

```python
# ä½¿ç”¨ TLS è¿æ¥ Redis
import redis

redis_client = redis.Redis(
    host='vlinders-redis',
    port=6379,
    ssl=True,
    ssl_cert_reqs='required',
    ssl_ca_certs='/etc/ssl/certs/ca.pem'
)

# ä½¿ç”¨ TLS è¿æ¥ PostgreSQL
DATABASE_URL = "postgresql://user:pass@host:5432/db?sslmode=require"
```

### å®‰å…¨æ‰«æ

```bash
# æ‰«æé•œåƒæ¼æ´
trivy image vlinders-server:latest

# æ‰«æ Kubernetes é…ç½®
kubesec scan deployment.yaml

# æ‰«æè¿è¡Œæ—¶å®‰å…¨
falco
```

---

## ğŸ“ è¿ç»´æ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥

- [ ] ç¡¬ä»¶èµ„æºå……è¶³ï¼ˆGPUã€CPUã€å†…å­˜ã€å­˜å‚¨ï¼‰
- [ ] è½¯ä»¶ç‰ˆæœ¬å…¼å®¹ï¼ˆCUDAã€Driverã€Kubernetesï¼‰
- [ ] ç½‘ç»œé…ç½®æ­£ç¡®ï¼ˆDNSã€é˜²ç«å¢™ã€è´Ÿè½½å‡è¡¡ï¼‰
- [ ] å­˜å‚¨å·å·²åˆ›å»ºï¼ˆæ¨¡å‹ã€æ—¥å¿—ã€å¤‡ä»½ï¼‰
- [ ] é…ç½®æ–‡ä»¶å·²å‡†å¤‡ï¼ˆConfigMapã€Secretï¼‰
- [ ] é•œåƒå·²æ„å»ºå¹¶æ¨é€åˆ°ä»“åº“
- [ ] SSL è¯ä¹¦å·²é…ç½®
- [ ] ç›‘æ§å‘Šè­¦å·²é…ç½®

### éƒ¨ç½²åæ£€æŸ¥

- [ ] æ‰€æœ‰ Pod è¿è¡Œæ­£å¸¸
- [ ] å¥åº·æ£€æŸ¥é€šè¿‡
- [ ] æœåŠ¡å¯è®¿é—®
- [ ] GPU æ­£å¸¸å·¥ä½œ
- [ ] æ—¥å¿—æ­£å¸¸è¾“å‡º
- [ ] ç›‘æ§æŒ‡æ ‡æ­£å¸¸
- [ ] å¤‡ä»½ä»»åŠ¡æ­£å¸¸è¿è¡Œ
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡

### æ—¥å¸¸è¿ç»´

- [ ] æ¯å¤©æ£€æŸ¥ç›‘æ§å‘Šè­¦
- [ ] æ¯å¤©æ£€æŸ¥æ—¥å¿—é”™è¯¯
- [ ] æ¯å‘¨æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
- [ ] æ¯å‘¨æ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§
- [ ] æ¯æœˆæ›´æ–°å®‰å…¨è¡¥ä¸
- [ ] æ¯æœˆè¿›è¡Œæ€§èƒ½æµ‹è¯•
- [ ] æ¯å­£åº¦è¿›è¡Œç¾éš¾æ¢å¤æ¼”ç»ƒ
- [ ] æ¯å¹´è¿›è¡Œå®‰å…¨å®¡è®¡

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- [vLLM Documentation](https://docs.vllm.ai/)
- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [Docker Documentation](https://docs.docker.com/)
- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)

### æœ€ä½³å®è·µ

- [Kubernetes Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)
- [GPU Operator Guide](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/)
- [Production Checklist](https://kubernetes.io/docs/setup/best-practices/)

### ç¤¾åŒºèµ„æº

- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [Kubernetes Slack](https://kubernetes.slack.com/)
- [NVIDIA Developer Forums](https://forums.developer.nvidia.com/)

---

**ä¸‹ä¸€æ­¥**: é˜…è¯» [08-å®‰å…¨ä¸åˆè§„.md](./08-å®‰å…¨ä¸åˆè§„.md)

**ç›¸å…³æ–‡æ¡£**:
- [03-vLLMé›†æˆæ–¹æ¡ˆ.md](./03-vLLMé›†æˆæ–¹æ¡ˆ.md)
- [04-Agentç¼–æ’ç³»ç»Ÿ.md](./04-Agentç¼–æ’ç³»ç»Ÿ.md)
- [05-APIè®¾è®¡è§„èŒƒ.md](./05-APIè®¾è®¡è§„èŒƒ.md)
