# Vlinders-Server è®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-28
**çŠ¶æ€**: ğŸ“ è®¾è®¡é˜¶æ®µ

---

## ğŸ“‹ æ¦‚è¿°

Vlinders-Server æ˜¯æœåŠ¡å™¨ç«¯çš„æ¨ç†å’Œç®¡ç†æ¡†æ¶ï¼ŒåŸºäº vLLM æä¾›é«˜æ€§èƒ½çš„å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œå¹¶å®ç° Agent ç¼–æ’ã€å·¥å…·æ‰§è¡Œã€ä»£ç åˆ†æç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚

### æ ¸å¿ƒå®šä½

> **æ¨ç†å±‚ + Agent ç¼–æ’å¼•æ“**
>
> ä¸ç›´æ¥é¢å‘ç”¨æˆ·ï¼Œåªæ¥å—æ¥è‡ª Vlinders-API çš„å†…éƒ¨è¯·æ±‚

### æŠ€æœ¯æ ˆ

```yaml
æ ¸å¿ƒæ¡†æ¶:
  - Python 3.11+
  - vLLM (å¤§æ¨¡å‹æ¨ç†)
  - FastAPI (HTTP æœåŠ¡)
  - gRPC (é«˜æ€§èƒ½å†…éƒ¨é€šä¿¡)

ä»£ç åˆ†æ:
  - Tree-sitter (è¯­æ³•è§£æ)
  - Pygments (è¯­æ³•é«˜äº®)

å‘é‡æœç´¢:
  - Qdrant (å‘é‡æ•°æ®åº“)
  - sentence-transformers (åµŒå…¥ç”Ÿæˆ)

ç¼“å­˜å’Œé˜Ÿåˆ—:
  - Redis (ç¼“å­˜ã€ä»»åŠ¡é˜Ÿåˆ—)
  - Celery (å¼‚æ­¥ä»»åŠ¡)

æ•°æ®åº“:
  - PostgreSQL (å…ƒæ•°æ®ã€æ—¥å¿—)

ç›‘æ§:
  - Prometheus (æŒ‡æ ‡)
  - Grafana (å¯è§†åŒ–)
  - OpenTelemetry (è¿½è¸ª)
```

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Vlinders-Server                               â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              API Gateway (FastAPI)                      â”‚    â”‚
â”‚  â”‚  - /internal/chat                                       â”‚    â”‚
â”‚  â”‚  - /internal/agent                                      â”‚    â”‚
â”‚  â”‚  - /internal/tools                                      â”‚    â”‚
â”‚  â”‚  - /internal/embeddings                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           Agent Orchestrator (æ ¸å¿ƒ)                     â”‚    â”‚
â”‚  â”‚  - Agent ç”Ÿå‘½å‘¨æœŸç®¡ç†                                   â”‚    â”‚
â”‚  â”‚  - å·¥å…·è°ƒç”¨å¾ªç¯                                         â”‚    â”‚
â”‚  â”‚  - å­ Agent å¹¶è¡Œæ‰§è¡Œ                                    â”‚    â”‚
â”‚  â”‚  - ä¸Šä¸‹æ–‡ç®¡ç†                                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â†“              â†“              â†“              â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  vLLM    â”‚  â”‚  Tool    â”‚  â”‚  Code    â”‚  â”‚ Context  â”‚      â”‚
â”‚  â”‚ Inferenceâ”‚  â”‚ Executor â”‚  â”‚ Analysis â”‚  â”‚ Manager  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â†“              â†“              â†“              â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  GPU     â”‚  â”‚ Sandbox  â”‚  â”‚Tree-sitterâ”‚ â”‚  Redis   â”‚      â”‚
â”‚  â”‚ Cluster  â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Knowledge Base                             â”‚    â”‚
â”‚  â”‚  - Qdrant (å‘é‡æ•°æ®åº“)                                  â”‚    â”‚
â”‚  â”‚  - PostgreSQL (å…ƒæ•°æ®)                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç›®å½•ç»“æ„

```
Vlinders-Server/
â”œâ”€â”€ README.md                          # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ docs/                              # æ–‡æ¡£
â”‚   â”œâ”€â”€ architecture.md                # æ¶æ„è®¾è®¡
â”‚   â”œâ”€â”€ vllm-setup.md                  # vLLM é…ç½®æŒ‡å—
â”‚   â”œâ”€â”€ agent-system.md                # Agent ç³»ç»Ÿè®¾è®¡
â”‚   â””â”€â”€ deployment.md                  # éƒ¨ç½²æŒ‡å—
â”œâ”€â”€ requirements.txt                   # Python ä¾èµ–
â”œâ”€â”€ pyproject.toml                     # é¡¹ç›®é…ç½®
â”œâ”€â”€ docker-compose.yml                 # Docker ç¼–æ’
â”œâ”€â”€ Dockerfile                         # Docker é•œåƒ
â”‚
â”œâ”€â”€ vlinders_server/                   # ä¸»ä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                        # å…¥å£æ–‡ä»¶
â”‚   â”œâ”€â”€ config.py                      # é…ç½®ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                           # API å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ internal.py                # å†…éƒ¨ API ç«¯ç‚¹
â”‚   â”‚   â”œâ”€â”€ health.py                  # å¥åº·æ£€æŸ¥
â”‚   â”‚   â””â”€â”€ middleware.py              # ä¸­é—´ä»¶
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/                     # æ¨ç†å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vllm_service.py            # vLLM æ¨ç†æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ model_manager.py           # æ¨¡å‹ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py          # Prompt æ„å»º
â”‚   â”‚   â””â”€â”€ streaming.py               # æµå¼å“åº”å¤„ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ agent/                         # Agent å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ orchestrator.py            # Agent ç¼–æ’å™¨
â”‚   â”‚   â”œâ”€â”€ tool_calling_loop.py       # å·¥å…·è°ƒç”¨å¾ªç¯
â”‚   â”‚   â”œâ”€â”€ agent_types.py             # Agent ç±»å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ subagent_executor.py       # å­ Agent æ‰§è¡Œå™¨
â”‚   â”‚   â””â”€â”€ hooks.py                   # Hook ç³»ç»Ÿ
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                         # å·¥å…·å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ executor.py                # å·¥å…·æ‰§è¡Œå™¨
â”‚   â”‚   â”œâ”€â”€ registry.py                # å·¥å…·æ³¨å†Œè¡¨
â”‚   â”‚   â”œâ”€â”€ sandbox.py                 # æ²™ç®±ç¯å¢ƒ
â”‚   â”‚   â””â”€â”€ builtin/                   # å†…ç½®å·¥å…·
â”‚   â”‚       â”œâ”€â”€ code_search.py
â”‚   â”‚       â”œâ”€â”€ semantic_search.py
â”‚   â”‚       â”œâ”€â”€ file_operations.py
â”‚   â”‚       â””â”€â”€ web_search.py
â”‚   â”‚
â”‚   â”œâ”€â”€ code_analysis/                 # ä»£ç åˆ†æå±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ parser.py                  # Tree-sitter è§£æå™¨
â”‚   â”‚   â”œâ”€â”€ symbol_extractor.py        # ç¬¦å·æå–
â”‚   â”‚   â”œâ”€â”€ dependency_analyzer.py     # ä¾èµ–åˆ†æ
â”‚   â”‚   â””â”€â”€ embeddings.py              # ä»£ç åµŒå…¥
â”‚   â”‚
â”‚   â”œâ”€â”€ context/                       # ä¸Šä¸‹æ–‡ç®¡ç†å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py                 # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ compressor.py              # å¯¹è¯å‹ç¼©
â”‚   â”‚   â”œâ”€â”€ memory.py                  # Memory æœåŠ¡
â”‚   â”‚   â””â”€â”€ cache.py                   # ç¼“å­˜ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge/                     # çŸ¥è¯†åº“å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py            # å‘é‡å­˜å‚¨
â”‚   â”‚   â”œâ”€â”€ indexer.py                 # ç´¢å¼•å™¨
â”‚   â”‚   â””â”€â”€ retriever.py               # æ£€ç´¢å™¨
â”‚   â”‚
â”‚   â””â”€â”€ utils/                         # å·¥å…·ç±»
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py                  # æ—¥å¿—
â”‚       â”œâ”€â”€ metrics.py                 # æŒ‡æ ‡
â”‚       â””â”€â”€ errors.py                  # é”™è¯¯å¤„ç†
â”‚
â”œâ”€â”€ tests/                             # æµ‹è¯•
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ performance/
â”‚
â”œâ”€â”€ scripts/                           # è„šæœ¬
â”‚   â”œâ”€â”€ setup_vllm.sh                  # vLLM å®‰è£…è„šæœ¬
â”‚   â”œâ”€â”€ download_models.py             # æ¨¡å‹ä¸‹è½½
â”‚   â””â”€â”€ benchmark.py                   # æ€§èƒ½æµ‹è¯•
â”‚
â””â”€â”€ configs/                           # é…ç½®æ–‡ä»¶
    â”œâ”€â”€ models.yaml                    # æ¨¡å‹é…ç½®
    â”œâ”€â”€ agents.yaml                    # Agent é…ç½®
    â””â”€â”€ tools.yaml                     # å·¥å…·é…ç½®
```

---

## ğŸš€ æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 1. vLLM æ¨ç†æœåŠ¡

**æ–‡ä»¶**: `vlinders_server/inference/vllm_service.py`

**èŒè´£**:
- åŠ è½½å’Œç®¡ç†å¤šä¸ªæ¨¡å‹
- æä¾›é«˜æ€§èƒ½æ¨ç†æ¥å£
- æ”¯æŒæµå¼å’Œéæµå¼ç”Ÿæˆ
- GPU èµ„æºç®¡ç†

**å…³é”®ç‰¹æ€§**:
- å¤š GPU å¹¶è¡Œï¼ˆTensor Parallelismï¼‰
- è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰
- PagedAttention ä¼˜åŒ–
- åŠ¨æ€æ‰¹å¤„ç†

**ç¤ºä¾‹ä»£ç **:
```python
from vllm import AsyncLLMEngine, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs

class VLLMInferenceService:
    """åŸºäº vLLM çš„æ¨ç†æœåŠ¡"""

    def __init__(self):
        self.engines: Dict[str, AsyncLLMEngine] = {}
        self.model_configs: Dict[str, ModelConfig] = {}

    async def load_model(
        self,
        model_name: str,
        model_path: str,
        tensor_parallel_size: int = 1,
        max_model_len: int = 32768,
        gpu_memory_utilization: float = 0.9
    ):
        """åŠ è½½æ¨¡å‹åˆ° vLLM"""

        engine_args = AsyncEngineArgs(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
            dtype="float16",
            max_model_len=max_model_len,
            gpu_memory_utilization=gpu_memory_utilization,
            trust_remote_code=True,
            enable_prefix_caching=True  # å¯ç”¨å‰ç¼€ç¼“å­˜
        )

        engine = AsyncLLMEngine.from_engine_args(engine_args)
        self.engines[model_name] = engine

        self.model_configs[model_name] = ModelConfig(
            name=model_name,
            path=model_path,
            max_tokens=max_model_len,
            tensor_parallel_size=tensor_parallel_size
        )

        logger.info(f"Model {model_name} loaded successfully")

    async def generate(
        self,
        model: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        top_p: float = 0.95,
        stream: bool = False,
        stop: Optional[List[str]] = None
    ):
        """ç”Ÿæˆæ–‡æœ¬"""

        engine = self.engines.get(model)
        if not engine:
            raise ValueError(f"Model {model} not loaded")

        sampling_params = SamplingParams(
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stop=stop
        )

        request_id = f"req_{uuid.uuid4().hex[:8]}"

        if stream:
            # æµå¼ç”Ÿæˆ
            async for output in engine.generate(
                prompt,
                sampling_params,
                request_id
            ):
                if output.outputs:
                    yield {
                        'text': output.outputs[0].text,
                        'finish_reason': output.outputs[0].finish_reason
                    }
        else:
            # éæµå¼ç”Ÿæˆ
            final_output = None
            async for output in engine.generate(
                prompt,
                sampling_params,
                request_id
            ):
                final_output = output

            if final_output and final_output.outputs:
                return {
                    'text': final_output.outputs[0].text,
                    'finish_reason': final_output.outputs[0].finish_reason,
                    'usage': {
                        'prompt_tokens': len(final_output.prompt_token_ids),
                        'completion_tokens': len(final_output.outputs[0].token_ids),
                        'total_tokens': len(final_output.prompt_token_ids) + len(final_output.outputs[0].token_ids)
                    }
                }
```

---

### 2. Agent ç¼–æ’å™¨

**æ–‡ä»¶**: `vlinders_server/agent/orchestrator.py`

**èŒè´£**:
- ç®¡ç† Agent ç”Ÿå‘½å‘¨æœŸ
- åè°ƒä¸» Agent å’Œå­ Agent
- æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯
- å¤„ç†å¹¶è¡Œæ‰§è¡Œ

**ç¤ºä¾‹ä»£ç **:
```python
class AgentOrchestrator:
    """Agent ç¼–æ’å™¨"""

    def __init__(
        self,
        inference_service: VLLMInferenceService,
        tool_executor: ToolExecutor,
        context_manager: ContextManager,
        code_analyzer: CodeAnalysisEngine
    ):
        self.inference = inference_service
        self.tools = tool_executor
        self.context = context_manager
        self.code_analyzer = code_analyzer

    async def run_agent(
        self,
        agent_type: str,
        instruction: str,
        context: Dict[str, Any],
        config: Optional[AgentConfig] = None
    ) -> AgentResult:
        """è¿è¡Œ Agent"""

        # 1. åˆ›å»º Agent ä¸Šä¸‹æ–‡
        agent_context = await self.context.create_context(
            agent_type=agent_type,
            instruction=instruction,
            workspace=context.get('workspace'),
            history=context.get('history', []),
            tools=context.get('tools', [])
        )

        # 2. é€‰æ‹©æ¨¡å‹
        model = self._select_model(agent_type, config)

        # 3. æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯
        loop_result = await self._run_tool_calling_loop(
            agent_context=agent_context,
            model=model,
            max_iterations=config.max_iterations if config else 10
        )

        # 4. å¤„ç†å­ Agent è°ƒç”¨
        if loop_result.subagent_calls:
            subagent_results = await self._execute_subagents_parallel(
                loop_result.subagent_calls,
                agent_context
            )
            loop_result.merge_subagent_results(subagent_results)

        # 5. å‹ç¼©ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if agent_context.should_compress():
            await self.context.compress_background(agent_context)

        return AgentResult(
            response=loop_result.final_response,
            tool_calls=loop_result.tool_calls,
            iterations=loop_result.iterations,
            usage=loop_result.usage
        )

    async def _run_tool_calling_loop(
        self,
        agent_context: AgentContext,
        model: str,
        max_iterations: int
    ) -> ToolCallingLoopResult:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¾ªç¯"""

        iteration = 0
        tool_call_rounds = []

        while iteration < max_iterations:
            # 1. æ„å»º Prompt
            prompt = await self._build_prompt(
                agent_context,
                tool_call_rounds
            )

            # 2. è°ƒç”¨æ¨¡å‹
            response = await self.inference.generate(
                model=model,
                prompt=prompt,
                max_tokens=4096,
                temperature=0.7,
                stream=False
            )

            # 3. è§£æå·¥å…·è°ƒç”¨
            tool_calls = self._parse_tool_calls(response['text'])

            if not tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¾ªç¯
                break

            # 4. æ‰§è¡Œå·¥å…·ï¼ˆå¹¶è¡Œï¼‰
            tool_results = await self.tools.execute_batch(tool_calls)

            # 5. è®°å½•æœ¬è½®
            tool_call_rounds.append({
                'iteration': iteration,
                'tool_calls': tool_calls,
                'tool_results': tool_results,
                'response': response['text']
            })

            # 6. æ›´æ–°ä¸Šä¸‹æ–‡
            agent_context.add_tool_round(tool_call_rounds[-1])

            iteration += 1

        return ToolCallingLoopResult(
            final_response=response['text'],
            tool_calls=tool_call_rounds,
            iterations=iteration,
            usage=response.get('usage', {})
        )

    async def _execute_subagents_parallel(
        self,
        subagent_calls: List[SubagentCall],
        parent_context: AgentContext
    ) -> List[SubagentResult]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå­ Agent"""

        tasks = [
            self.run_agent(
                agent_type=call.agent_type,
                instruction=call.instruction,
                context={
                    'workspace': parent_context.workspace,
                    'parent_agent_id': parent_context.agent_id
                }
            )
            for call in subagent_calls
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        return [
            SubagentResult(
                agent_type=call.agent_type,
                result=result if not isinstance(result, Exception) else None,
                error=str(result) if isinstance(result, Exception) else None
            )
            for call, result in zip(subagent_calls, results)
        ]
```

---

### 3. å·¥å…·æ‰§è¡Œå™¨

**æ–‡ä»¶**: `vlinders_server/tools/executor.py`

**èŒè´£**:
- æ‰§è¡Œå·¥å…·è°ƒç”¨
- æ²™ç®±éš”ç¦»
- å¹¶è¡Œæ‰§è¡Œ
- é”™è¯¯å¤„ç†

**ç¤ºä¾‹ä»£ç **:
```python
class ToolExecutor:
    """å·¥å…·æ‰§è¡Œå™¨"""

    def __init__(self, sandbox: Sandbox):
        self.tools: Dict[str, Tool] = {}
        self.sandbox = sandbox
        self.register_builtin_tools()

    def register_builtin_tools(self):
        """æ³¨å†Œå†…ç½®å·¥å…·"""
        from .builtin import (
            CodeSearchTool,
            SemanticSearchTool,
            FileOperationsTool,
            WebSearchTool
        )

        self.tools['search_code'] = CodeSearchTool()
        self.tools['semantic_search'] = SemanticSearchTool()
        self.tools['read_file'] = FileOperationsTool()
        self.tools['web_search'] = WebSearchTool()

    async def execute(
        self,
        tool_name: str,
        arguments: Dict[str, Any],
        context: Optional[ExecutionContext] = None
    ) -> ToolResult:
        """æ‰§è¡Œå•ä¸ªå·¥å…·"""

        tool = self.tools.get(tool_name)
        if not tool:
            raise ToolNotFoundError(f"Tool {tool_name} not found")

        # 1. éªŒè¯å‚æ•°
        validated_args = tool.validate_arguments(arguments)

        # 2. æ£€æŸ¥æƒé™
        if context and not self._check_permission(tool, context):
            raise PermissionError(f"Not allowed to use {tool_name}")

        # 3. åœ¨æ²™ç®±ä¸­æ‰§è¡Œ
        try:
            result = await self.sandbox.execute(
                tool=tool,
                arguments=validated_args,
                timeout=tool.timeout,
                resource_limits=tool.resource_limits
            )

            return ToolResult(
                tool_name=tool_name,
                success=True,
                result=result,
                duration=result.duration
            )

        except Exception as e:
            logger.error(f"Tool {tool_name} execution failed: {e}")
            return ToolResult(
                tool_name=tool_name,
                success=False,
                error=str(e)
            )

    async def execute_batch(
        self,
        tool_calls: List[ToolCall],
        context: Optional[ExecutionContext] = None
    ) -> List[ToolResult]:
        """æ‰¹é‡æ‰§è¡Œå·¥å…·ï¼ˆå¹¶è¡Œï¼‰"""

        tasks = [
            self.execute(call.name, call.arguments, context)
            for call in tool_calls
        ]

        return await asyncio.gather(*tasks, return_exceptions=True)
```

---

## ğŸ“¡ å†…éƒ¨ API è®¾è®¡

**æ–‡ä»¶**: `vlinders_server/api/internal.py`

```python
from fastapi import FastAPI, Header, HTTPException, Depends
from fastapi.responses import StreamingResponse

app = FastAPI(title="Vlinders-Server Internal API")

# å†…éƒ¨è®¤è¯
INTERNAL_SECRET = os.getenv('INTERNAL_SECRET')

def verify_internal_auth(x_internal_auth: str = Header(...)):
    """éªŒè¯å†…éƒ¨è¯·æ±‚"""
    if x_internal_auth != INTERNAL_SECRET:
        raise HTTPException(status_code=403, detail="Forbidden")

@app.post("/internal/chat")
async def internal_chat(
    request: InternalChatRequest,
    auth: str = Depends(verify_internal_auth)
):
    """å†…éƒ¨èŠå¤©æ¥å£"""

    # è¿è¡Œ Agent
    result = await agent_orchestrator.run_agent(
        agent_type='chat',
        instruction=request.messages[-1]['content'],
        context={
            'messages': request.messages,
            'tools': request.tools,
            'user_id': request.user_id
        }
    )

    # è¿”å›ç»“æœ
    return {
        'id': f"chatcmpl_{uuid.uuid4().hex[:8]}",
        'model': request.model,
        'choices': [{
            'message': {
                'role': 'assistant',
                'content': result.response
            },
            'finish_reason': 'stop'
        }],
        'usage': result.usage
    }

@app.post("/internal/chat/stream")
async def internal_chat_stream(
    request: InternalChatRequest,
    auth: str = Depends(verify_internal_auth)
):
    """å†…éƒ¨èŠå¤©æ¥å£ï¼ˆæµå¼ï¼‰"""

    async def generate():
        async for chunk in agent_orchestrator.run_agent_stream(
            agent_type='chat',
            instruction=request.messages[-1]['content'],
            context={
                'messages': request.messages,
                'tools': request.tools
            }
        ):
            yield f"data: {json.dumps(chunk)}\n\n"

        yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

@app.post("/internal/agent")
async def internal_agent(
    request: InternalAgentRequest,
    auth: str = Depends(verify_internal_auth)
):
    """å†…éƒ¨ Agent æ¥å£"""

    result = await agent_orchestrator.run_agent(
        agent_type=request.agent_type,
        instruction=request.instruction,
        context=request.context,
        config=request.config
    )

    return {
        'agent_id': result.agent_id,
        'response': result.response,
        'tool_calls': result.tool_calls,
        'iterations': result.iterations,
        'usage': result.usage
    }

@app.post("/internal/tools/execute")
async def internal_tool_execute(
    request: InternalToolRequest,
    auth: str = Depends(verify_internal_auth)
):
    """å†…éƒ¨å·¥å…·æ‰§è¡Œæ¥å£"""

    result = await tool_executor.execute(
        tool_name=request.tool_name,
        arguments=request.arguments
    )

    return {
        'success': result.success,
        'result': result.result,
        'error': result.error,
        'duration': result.duration
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        'status': 'healthy',
        'models_loaded': list(vllm_service.engines.keys()),
        'gpu_available': torch.cuda.is_available(),
        'gpu_count': torch.cuda.device_count()
    }
```

---

## ğŸ”§ é…ç½®ç®¡ç†

**æ–‡ä»¶**: `configs/models.yaml`

```yaml
models:
  - name: vlinders-gpt-4
    path: /models/llama-3-70b
    tensor_parallel_size: 4
    max_model_len: 32768
    gpu_memory_utilization: 0.9
    enabled: true

  - name: vlinders-gpt-3.5
    path: /models/llama-3-8b
    tensor_parallel_size: 1
    max_model_len: 16384
    gpu_memory_utilization: 0.8
    enabled: true

  - name: vlinders-code
    path: /models/codellama-34b
    tensor_parallel_size: 2
    max_model_len: 16384
    gpu_memory_utilization: 0.85
    enabled: true
```

**æ–‡ä»¶**: `configs/agents.yaml`

```yaml
agents:
  plan:
    model: vlinders-gpt-4
    max_iterations: 5
    tools:
      - search_code
      - read_file
      - semantic_search
    subagents:
      - explore

  explore:
    model: vlinders-gpt-3.5  # ä½¿ç”¨å°æ¨¡å‹
    max_iterations: 3
    tools:
      - search_code
      - read_file

  edit:
    model: vlinders-gpt-4
    max_iterations: 10
    tools:
      - search_code
      - read_file
      - edit_file
      - run_test
```

---

## ğŸš€ éƒ¨ç½²æ–¹æ¡ˆ

### Docker Compose

**æ–‡ä»¶**: `docker-compose.yml`

```yaml
version: '3.8'

services:
  vlinders-server:
    build: .
    ports:
      - "8000:8000"
    environment:
      - INTERNAL_SECRET=${INTERNAL_SECRET}
      - REDIS_URL=redis://redis:6379
      - POSTGRES_URL=postgresql://postgres:password@postgres:5432/vlinders
      - QDRANT_URL=http://qdrant:6333
    volumes:
      - ./models:/models
      - ./data:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 4
              capabilities: [gpu]

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=vlinders
    volumes:
      - postgres_data:/var/lib/postgresql/data

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

volumes:
  postgres_data:
  qdrant_data:
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### ç›®æ ‡æ€§èƒ½

| æŒ‡æ ‡ | ç›®æ ‡å€¼ |
|------|--------|
| é¦– Token å»¶è¿Ÿ (TTFT) | < 500ms |
| Token ç”Ÿæˆé€Ÿåº¦ | > 50 tokens/s |
| å¹¶å‘è¯·æ±‚æ•° | > 100 |
| GPU åˆ©ç”¨ç‡ | > 80% |
| å†…å­˜ä½¿ç”¨ | < 90% |

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. âœ… å®ç° vLLM æ¨ç†æœåŠ¡
2. âœ… å®ç° Agent ç¼–æ’å™¨
3. âœ… å®ç°å·¥å…·æ‰§è¡Œå™¨
4. âœ… å®ç°ä»£ç åˆ†æå¼•æ“
5. âœ… éƒ¨ç½²å’Œæµ‹è¯•

---

**çŠ¶æ€**: ğŸ“ è®¾è®¡å®Œæˆ
**ä¸‹ä¸€æ­¥**: å¼€å§‹å®æ–½
