# ç®¡ç†æ¡†æ¶è®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2026-02-28
**è´Ÿè´£äºº**: DevOps Team
**çŠ¶æ€**: ğŸ“ è®¾è®¡é˜¶æ®µ

---

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿° Vlinders Platform çš„ç®¡ç†æ¡†æ¶ï¼ŒåŒ…æ‹¬ç›‘æ§ã€æ—¥å¿—ã€è¿½è¸ªã€å‘Šè­¦ç­‰å®Œæ•´çš„å¯è§‚æµ‹æ€§è§£å†³æ–¹æ¡ˆã€‚

### è®¾è®¡ç›®æ ‡

1. **å…¨æ ˆå¯è§‚æµ‹æ€§**: Metrics + Logs + Traces ä¸‰ä½ä¸€ä½“
2. **å®æ—¶ç›‘æ§**: ç§’çº§æ•°æ®é‡‡é›†å’Œå±•ç¤º
3. **æ™ºèƒ½å‘Šè­¦**: å‡†ç¡®çš„å¼‚å¸¸æ£€æµ‹å’Œé€šçŸ¥
4. **é—®é¢˜å®šä½**: å¿«é€Ÿå®šä½å’Œè¯Šæ–­é—®é¢˜
5. **æˆæœ¬åˆ†æ**: èµ„æºä½¿ç”¨å’Œæˆæœ¬è¿½è¸ª

---

## ğŸ—ï¸ å¯è§‚æµ‹æ€§æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¯è§‚æµ‹æ€§æ¶æ„                                   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              åº”ç”¨å±‚                                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚  â”‚  â”‚ API      â”‚  â”‚ Inferenceâ”‚  â”‚ User Svc â”‚            â”‚    â”‚
â”‚  â”‚  â”‚ Gateway  â”‚  â”‚ Service  â”‚  â”‚          â”‚            â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚  â”‚       â”‚              â”‚              â”‚                  â”‚    â”‚
â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚    â”‚
â”‚  â”‚                      â†“                                  â”‚    â”‚
â”‚  â”‚         OpenTelemetry SDK (è‡ªåŠ¨åŸ‹ç‚¹)                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         OpenTelemetry Collector (é‡‡é›†å±‚)                â”‚    â”‚
â”‚  â”‚  - æ¥æ”¶ Traces/Metrics/Logs                            â”‚    â”‚
â”‚  â”‚  - æ•°æ®å¤„ç†å’Œè½¬æ¢                                        â”‚    â”‚
â”‚  â”‚  - è·¯ç”±åˆ°ä¸åŒåç«¯                                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚              â”‚              â”‚                          â”‚
â”‚         â†“              â†“              â†“                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Tempo    â”‚  â”‚Prometheusâ”‚  â”‚  Loki    â”‚                     â”‚
â”‚  â”‚ (Traces) â”‚  â”‚(Metrics) â”‚  â”‚  (Logs)  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚              â”‚              â”‚                          â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                      â†“                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Grafana (å¯è§†åŒ–å±‚)                         â”‚    â”‚
â”‚  â”‚  - ç»Ÿä¸€ä»ªè¡¨æ¿                                            â”‚    â”‚
â”‚  â”‚  - å‘Šè­¦ç®¡ç†                                              â”‚    â”‚
â”‚  â”‚  - æŸ¥è¯¢å’Œåˆ†æ                                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å‚è€ƒ**: [OpenTelemetry + Prometheus + Grafana](https://grafana.com/blog/2023/07/20/a-practical-guide-to-data-collection-with-opentelemetry-and-prometheus/)

---

## ğŸ“Š Metrics (æŒ‡æ ‡ç›‘æ§)

### Prometheus éƒ¨ç½²

```yaml
# prometheus-values.yaml
server:
  global:
    scrape_interval: 15s
    evaluation_interval: 15s
    external_labels:
      cluster: 'vlinders-prod'
      environment: 'production'

  retention: 15d  # ä¿ç•™ 15 å¤©

  persistentVolume:
    enabled: true
    size: 100Gi

  resources:
    limits:
      cpu: 2000m
      memory: 8Gi
    requests:
      cpu: 1000m
      memory: 4Gi

  # Scrape é…ç½®
  scrapeConfigs:
    # Kubernetes èŠ‚ç‚¹
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        - source_labels: [__address__]
          regex: '(.*):10250'
          replacement: '${1}:9100'
          target_label: __address__

    # Kubernetes Pods
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true

    # Ray Serve
    - job_name: 'ray-serve'
      static_configs:
        - targets: ['ray-head:8000']

    # vLLM
    - job_name: 'vllm'
      static_configs:
        - targets: ['vllm-exporter:9090']

alertmanager:
  enabled: true
  persistentVolume:
    enabled: true
    size: 10Gi
```

### æ ¸å¿ƒæŒ‡æ ‡å®šä¹‰

```python
# metrics.py
from prometheus_client import Counter, Histogram, Gauge, Info
from functools import wraps
import time

# ==================== ä¸šåŠ¡æŒ‡æ ‡ ====================

# è¯·æ±‚æŒ‡æ ‡
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

# æ¨ç†æŒ‡æ ‡
inference_requests_total = Counter(
    'inference_requests_total',
    'Total inference requests',
    ['model', 'tenant_id', 'status']
)

inference_latency_seconds = Histogram(
    'inference_latency_seconds',
    'Inference latency',
    ['model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

inference_tokens_total = Counter(
    'inference_tokens_total',
    'Total tokens generated',
    ['model', 'tenant_id']
)

# é˜Ÿåˆ—æŒ‡æ ‡
inference_queue_size = Gauge(
    'inference_queue_size',
    'Number of requests in queue',
    ['model']
)

# ==================== ç³»ç»ŸæŒ‡æ ‡ ====================

# GPU æŒ‡æ ‡
gpu_utilization_percent = Gauge(
    'gpu_utilization_percent',
    'GPU utilization percentage',
    ['gpu_id', 'node']
)

gpu_memory_used_bytes = Gauge(
    'gpu_memory_used_bytes',
    'GPU memory used in bytes',
    ['gpu_id', 'node']
)

gpu_temperature_celsius = Gauge(
    'gpu_temperature_celsius',
    'GPU temperature',
    ['gpu_id', 'node']
)

# Ray Serve æŒ‡æ ‡
ray_serve_deployment_replicas = Gauge(
    'ray_serve_deployment_replicas',
    'Number of deployment replicas',
    ['deployment']
)

ray_serve_deployment_queued_queries = Gauge(
    'ray_serve_deployment_queued_queries',
    'Number of queued queries',
    ['deployment']
)

# ==================== ä¸šåŠ¡æŒ‡æ ‡ ====================

# ç”¨æˆ·æŒ‡æ ‡
active_users_total = Gauge(
    'active_users_total',
    'Number of active users',
    ['tenant_id']
)

# è®¢é˜…æŒ‡æ ‡
subscriptions_total = Gauge(
    'subscriptions_total',
    'Total subscriptions',
    ['plan', 'status']
)

# æ”¶å…¥æŒ‡æ ‡
revenue_total = Counter(
    'revenue_total',
    'Total revenue in cents',
    ['plan', 'currency']
)

# ==================== è£…é¥°å™¨ ====================

def track_request_metrics(func):
    """è¿½è¸ª HTTP è¯·æ±‚æŒ‡æ ‡"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        status = 200

        try:
            result = await func(*args, **kwargs)
            return result
        except Exception as e:
            status = 500
            raise
        finally:
            duration = time.time() - start_time
            http_requests_total.labels(
                method=request.method,
                endpoint=request.url.path,
                status=status
            ).inc()
            http_request_duration_seconds.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(duration)

    return wrapper

def track_inference_metrics(func):
    """è¿½è¸ªæ¨ç†æŒ‡æ ‡"""
    @wraps(func)
    async def wrapper(model: str, tenant_id: str, *args, **kwargs):
        start_time = time.time()
        status = "success"

        try:
            result = await func(model, tenant_id, *args, **kwargs)

            # è®°å½• token ä½¿ç”¨
            if "usage" in result:
                inference_tokens_total.labels(
                    model=model,
                    tenant_id=tenant_id
                ).inc(result["usage"]["total_tokens"])

            return result

        except Exception as e:
            status = "error"
            raise

        finally:
            duration = time.time() - start_time

            inference_requests_total.labels(
                model=model,
                tenant_id=tenant_id,
                status=status
            ).inc()

            inference_latency_seconds.labels(
                model=model
            ).observe(duration)

    return wrapper
```

### GPU ç›‘æ§

```python
# gpu_exporter.py
import pynvml
from prometheus_client import start_http_server
import time

class GPUExporter:
    """GPU æŒ‡æ ‡å¯¼å‡ºå™¨"""

    def __init__(self):
        pynvml.nvmlInit()
        self.device_count = pynvml.nvmlDeviceGetCount()

    def collect_metrics(self):
        """é‡‡é›† GPU æŒ‡æ ‡"""

        for i in range(self.device_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)

            # åˆ©ç”¨ç‡
            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
            gpu_utilization_percent.labels(
                gpu_id=str(i),
                node=os.environ.get("NODE_NAME", "unknown")
            ).set(utilization.gpu)

            # å†…å­˜
            memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
            gpu_memory_used_bytes.labels(
                gpu_id=str(i),
                node=os.environ.get("NODE_NAME", "unknown")
            ).set(memory.used)

            # æ¸©åº¦
            temperature = pynvml.nvmlDeviceGetTemperature(
                handle,
                pynvml.NVML_TEMPERATURE_GPU
            )
            gpu_temperature_celsius.labels(
                gpu_id=str(i),
                node=os.environ.get("NODE_NAME", "unknown")
            ).set(temperature)

    def run(self, port=9090):
        """å¯åŠ¨å¯¼å‡ºå™¨"""
        start_http_server(port)

        while True:
            self.collect_metrics()
            time.sleep(15)  # æ¯ 15 ç§’é‡‡é›†ä¸€æ¬¡

if __name__ == "__main__":
    exporter = GPUExporter()
    exporter.run()
```

---

## ğŸ“ Logs (æ—¥å¿—ç®¡ç†)

### Loki éƒ¨ç½²

```yaml
# loki-values.yaml
loki:
  auth_enabled: false

  server:
    http_listen_port: 3100

  ingester:
    lifecycler:
      ring:
        kvstore:
          store: inmemory
        replication_factor: 1
    chunk_idle_period: 5m
    chunk_retain_period: 30s

  schema_config:
    configs:
      - from: 2024-01-01
        store: boltdb-shipper
        object_store: s3
        schema: v11
        index:
          prefix: loki_index_
          period: 24h

  storage_config:
    boltdb_shipper:
      active_index_directory: /loki/index
      cache_location: /loki/cache
      shared_store: s3
    aws:
      s3: s3://us-east-1/vlinders-logs
      s3forcepathstyle: true

  limits_config:
    enforce_metric_name: false
    reject_old_samples: true
    reject_old_samples_max_age: 168h  # 7 days
    ingestion_rate_mb: 10
    ingestion_burst_size_mb: 20

  chunk_store_config:
    max_look_back_period: 0s

  table_manager:
    retention_deletes_enabled: true
    retention_period: 2160h  # 90 days

promtail:
  enabled: true
  config:
    clients:
      - url: http://loki:3100/loki/api/v1/push
```

### ç»“æ„åŒ–æ—¥å¿—

```python
# logging_config.py
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    """JSON æ ¼å¼åŒ–å™¨"""

    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }

        # æ·»åŠ é¢å¤–å­—æ®µ
        if hasattr(record, "tenant_id"):
            log_data["tenant_id"] = record.tenant_id

        if hasattr(record, "user_id"):
            log_data["user_id"] = record.user_id

        if hasattr(record, "request_id"):
            log_data["request_id"] = record.request_id

        # æ·»åŠ å¼‚å¸¸ä¿¡æ¯
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)

        return json.dumps(log_data)

def setup_logging():
    """é…ç½®æ—¥å¿—"""

    handler = logging.StreamHandler()
    handler.setFormatter(JSONFormatter())

    logger = logging.getLogger()
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

    return logger

# ä½¿ç”¨ç¤ºä¾‹
logger = setup_logging()

logger.info(
    "Inference request completed",
    extra={
        "tenant_id": "tenant_123",
        "user_id": "user_456",
        "request_id": "req_789",
        "model": "gpt-4",
        "tokens": 1024,
        "latency_ms": 523
    }
)
```

### æ—¥å¿—æŸ¥è¯¢

```python
# log_query.py
import requests

class LokiClient:
    """Loki å®¢æˆ·ç«¯"""

    def __init__(self, url="http://loki:3100"):
        self.url = url

    def query(
        self,
        query: str,
        start: str = "1h",
        limit: int = 100
    ):
        """æŸ¥è¯¢æ—¥å¿—"""

        response = requests.get(
            f"{self.url}/loki/api/v1/query_range",
            params={
                "query": query,
                "start": start,
                "limit": limit
            }
        )

        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
client = LokiClient()

# æŸ¥è¯¢ç‰¹å®šç§Ÿæˆ·çš„é”™è¯¯æ—¥å¿—
logs = client.query(
    query='{tenant_id="tenant_123"} |= "ERROR"',
    start="24h"
)

# æŸ¥è¯¢æ¨ç†å»¶è¿Ÿ > 1s çš„æ—¥å¿—
logs = client.query(
    query='{job="inference"} | json | latency_ms > 1000',
    start="1h"
)
```

---

## ğŸ” Traces (åˆ†å¸ƒå¼è¿½è¸ª)

### Tempo éƒ¨ç½²

```yaml
# tempo-values.yaml
tempo:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

  storage:
    trace:
      backend: s3
      s3:
        bucket: vlinders-traces
        endpoint: s3.amazonaws.com
        region: us-east-1

  retention: 720h  # 30 days

  resources:
    limits:
      cpu: 2000m
      memory: 4Gi
    requests:
      cpu: 1000m
      memory: 2Gi
```

### OpenTelemetry é›†æˆ

```python
# tracing.py
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
from opentelemetry.instrumentation.asyncpg import AsyncPGInstrumentor

def setup_tracing(service_name: str):
    """é…ç½®åˆ†å¸ƒå¼è¿½è¸ª"""

    # åˆ›å»º TracerProvider
    provider = TracerProvider(
        resource=Resource.create({
            "service.name": service_name,
            "service.version": "1.0.0",
            "deployment.environment": "production"
        })
    )

    # é…ç½® OTLP å¯¼å‡ºå™¨
    otlp_exporter = OTLPSpanExporter(
        endpoint="http://tempo:4317",
        insecure=True
    )

    # æ·»åŠ  Span å¤„ç†å™¨
    provider.add_span_processor(
        BatchSpanProcessor(otlp_exporter)
    )

    # è®¾ç½®å…¨å±€ TracerProvider
    trace.set_tracer_provider(provider)

    return trace.get_tracer(service_name)

# è‡ªåŠ¨åŸ‹ç‚¹
def instrument_app(app: FastAPI):
    """è‡ªåŠ¨åŸ‹ç‚¹ FastAPI åº”ç”¨"""

    # FastAPI åŸ‹ç‚¹
    FastAPIInstrumentor.instrument_app(app)

    # HTTP å®¢æˆ·ç«¯åŸ‹ç‚¹
    HTTPXClientInstrumentor().instrument()

    # æ•°æ®åº“åŸ‹ç‚¹
    AsyncPGInstrumentor().instrument()

# æ‰‹åŠ¨åŸ‹ç‚¹ç¤ºä¾‹
tracer = setup_tracing("inference-service")

@app.post("/inference")
async def inference(request: InferenceRequest):
    with tracer.start_as_current_span("inference") as span:
        # æ·»åŠ å±æ€§
        span.set_attribute("model", request.model)
        span.set_attribute("tenant_id", request.tenant_id)

        # å­ Span
        with tracer.start_as_current_span("load_model"):
            model = await load_model(request.model)

        with tracer.start_as_current_span("generate"):
            result = await model.generate(request.prompt)

        # è®°å½•äº‹ä»¶
        span.add_event("inference_completed", {
            "tokens": result["usage"]["total_tokens"]
        })

        return result
```

---

## ğŸš¨ å‘Šè­¦ç³»ç»Ÿ

### Alertmanager é…ç½®

```yaml
# alertmanager-config.yaml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/xxx'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

  routes:
    # ä¸¥é‡å‘Šè­¦ -> PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty'
      continue: true

    # è­¦å‘Š -> Slack
    - match:
        severity: warning
      receiver: 'slack'

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'xxx'

  - name: 'slack'
    slack_configs:
      - channel: '#warnings'
```

### å‘Šè­¦è§„åˆ™

```yaml
# alert-rules.yaml
groups:
  - name: inference_alerts
    interval: 30s
    rules:
      # é«˜å»¶è¿Ÿå‘Šè­¦
      - alert: HighInferenceLatency
        expr: |
          histogram_quantile(0.95,
            rate(inference_latency_seconds_bucket[5m])
          ) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High inference latency"
          description: "P95 latency is {{ $value }}s"

      # GPU åˆ©ç”¨ç‡ä½
      - alert: LowGPUUtilization
        expr: avg(gpu_utilization_percent) < 30
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Low GPU utilization"
          description: "GPU utilization is {{ $value }}%"

      # æ¨ç†å¤±è´¥ç‡é«˜
      - alert: HighInferenceErrorRate
        expr: |
          rate(inference_requests_total{status="error"}[5m])
          /
          rate(inference_requests_total[5m])
          > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High inference error rate"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # é˜Ÿåˆ—ç§¯å‹
      - alert: InferenceQueueBacklog
        expr: inference_queue_size > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Inference queue backlog"
          description: "Queue size is {{ $value }}"

  - name: system_alerts
    interval: 30s
    rules:
      # GPU æ¸©åº¦è¿‡é«˜
      - alert: HighGPUTemperature
        expr: gpu_temperature_celsius > 85
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High GPU temperature"
          description: "GPU {{ $labels.gpu_id }} temperature is {{ $value }}Â°C"

      # å†…å­˜ä½¿ç”¨ç‡é«˜
      - alert: HighMemoryUsage
        expr: |
          (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)
          /
          node_memory_MemTotal_bytes
          > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}"

      # Pod é‡å¯é¢‘ç¹
      - alert: PodRestartingFrequently
        expr: rate(kube_pod_container_status_restarts_total[1h]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod restarting frequently"
          description: "Pod {{ $labels.pod }} restarted {{ $value }} times"
```

---

## ğŸ“Š Grafana ä»ªè¡¨æ¿

### ä¸»ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "Vlinders Platform Overview",
    "tags": ["overview"],
    "timezone": "browser",
    "panels": [
      {
        "title": "Requests per Second",
        "type": "graph",
        "targets": [{
          "expr": "sum(rate(http_requests_total[5m]))"
        }]
      },
      {
        "title": "P95 Latency",
        "type": "graph",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
        }]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [{
          "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))"
        }]
      },
      {
        "title": "Active Users",
        "type": "stat",
        "targets": [{
          "expr": "sum(active_users_total)"
        }]
      },
      {
        "title": "GPU Utilization",
        "type": "gauge",
        "targets": [{
          "expr": "avg(gpu_utilization_percent)"
        }]
      },
      {
        "title": "Inference Throughput",
        "type": "graph",
        "targets": [{
          "expr": "sum(rate(inference_requests_total[5m])) by (model)"
        }]
      }
    ]
  }
}
```

---

## ğŸ’° æˆæœ¬åˆ†æ

### æˆæœ¬è¿½è¸ª

```python
# cost_tracker.py
class CostTracker:
    """æˆæœ¬è¿½è¸ª"""

    # æˆæœ¬é…ç½®
    COSTS = {
        "gpu_a100_80gb_hour": 3.06,  # USD per hour
        "cpu_core_hour": 0.05,
        "memory_gb_hour": 0.01,
        "storage_gb_month": 0.10,
        "network_gb": 0.09,
    }

    async def calculate_inference_cost(
        self,
        model: str,
        duration_seconds: float,
        gpu_count: int = 1
    ) -> float:
        """è®¡ç®—æ¨ç†æˆæœ¬"""

        hours = duration_seconds / 3600
        gpu_cost = hours * gpu_count * self.COSTS["gpu_a100_80gb_hour"]

        return gpu_cost

    async def get_tenant_cost(
        self,
        tenant_id: str,
        start_date: date,
        end_date: date
    ) -> Dict:
        """è·å–ç§Ÿæˆ·æˆæœ¬"""

        # æŸ¥è¯¢ä½¿ç”¨é‡
        usage = await self.db.fetch(
            """
            SELECT
                resource_type,
                SUM(quantity) as total
            FROM usage_records
            WHERE tenant_id = $1
              AND recorded_at BETWEEN $2 AND $3
            GROUP BY resource_type
            """,
            tenant_id,
            start_date,
            end_date
        )

        # è®¡ç®—æˆæœ¬
        total_cost = 0
        breakdown = {}

        for record in usage:
            if record["resource_type"] == "gpu_seconds":
                cost = (record["total"] / 3600) * self.COSTS["gpu_a100_80gb_hour"]
                breakdown["gpu"] = cost
                total_cost += cost

        return {
            "total_cost": total_cost,
            "breakdown": breakdown,
            "period": {
                "start": start_date.isoformat(),
                "end": end_date.isoformat()
            }
        }
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [OpenTelemetry + Prometheus + Grafana](https://grafana.com/blog/2023/07/20/a-practical-guide-to-data-collection-with-opentelemetry-and-prometheus/)
- [FastAPI Observability](https://blueswen.hashnode.dev/enable-observability-for-fastapi-service-with-opentelemetry-prometheus-and-grafana)
- [Kubernetes Observability](https://www.stackgenie.io/kubernetes-observability-prometheus-opentelemetry-grafana/)

---

**çŠ¶æ€**: ğŸ“ è®¾è®¡å®Œæˆ
**ä¸‹ä¸€æ­¥**: å®æ–½å’Œé›†æˆæµ‹è¯•
